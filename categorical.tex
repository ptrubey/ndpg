\section{Binary and Categorical Data}
Many applications for which novelty detection methods might be applied will 
    feature both real and categorical data.  \makenote{rewrite} The most apparent 
    shortcoming of the projected Gamma model developed in~\cite{trubey:pg} is
    its inability to handle binary or categorical data.  In order to ameliorate this
    shortcoming, and as an added bonus propose a new method for anomaly detection in
    categorical data, we propose the following.

Suppose $C$ is a vector of $M$ random categorical variables.  Then $\bm{C}_{m}$ 
    is a random categorical variable, with $K_{m} \geq 2$ categories. Regard $\bm{W}$ 
    as $\bm{C}$, recoded in one-hot encoding.  That is, $\bm{W}$ is a binary vector of 
    length $K = \sum_{m = 1}^M K_{m}$, $\sum_{k = 1}^K W_k = M$, and every $m$ subset 
    of $\bm{W}$ sums to 1.  This corresponds to multinomial encoding for $\bm{C}_m$,
    with size 1. We then consider a Dirichlet-multinomial density for $\bm{W}_m$,
    established as
  \begin{equation*}
    \bm{w}_{im}\mid\bm{\alpha} 
    \sim 
    \int_{\pi_{im}} 
    \text{Multinom}(\bm{w}_{im}\mid\pi_{im})
    \text{Dir}(\pi\mid\bm{\alpha})\text{d}\pi_{im}.
  \end{equation*}
  Recall that the Dirichlet distribution is a special case of the projected gamma,
  where the rate parameters are uniformly fixed $\beta_{\ell} = \beta$.  We investigated
  the general $\mathcal{PG}_1$ to replace the Dirichlet, but a closed-form solution to 
  the above integral was not available.  That said, we observe in 
  \cite{trubey:pg}, that as a kernel function in a Dirichlet process mixture, there was
  no demonstrable performance advantage to allowing the rate parameters to vary.
 
  For simplicity of notation, consider, a \emph{concatenated} Dirichlet-multinomial 
  ($\mathcal{CDM}$) as a product of Dirichlet-multinomial densities.  That is, 
  $\mathcal{CDM}(\bm{w}\mid\bm{\alpha}) = \prod_{m = 1}^M\mathcal{DM}(\bm{w}_m\mid\bm{\alpha}_m)$.
  Then we establish a Bayesian non-parametric categorical data model as
  \begin{equation}
    \begin{aligned}
      \bm{W}_i &\mid \bm{\alpha}_i \sim \mathcal{CDM}\left(\bm{W}_i\mid\bm{\alpha}_i\right)\\
      \bm{\alpha_i} &\sim G\\
      G\sim &\mathcal{DP}\left(\eta, G_0\right)
    \end{aligned}
    ~
    \begin{aligned}
    G_0 &= \mathcal{LN}\left(\bm{\alpha}\mid\bm{\mu},\Sigma\right)\\
    \mu &\sim \mathcal{N}\left(\bm{0},\bm{1}\right)\\
    \Sigma &\sim \mathcal{IW}\left(\nu, \Psi\right)
    \end{aligned}
  \end{equation}
   
  
  


\makenote{This is way too long.  rewrite.}
Let $C$ be a random categorical variable, with $K \geq 2$ categories.  We 
    re-code $C$ into the random vector $\bm{W}$ with length $K$, taking the form
    \begin{equation*}
        W_k = \begin{cases}
            1 &\text{if }C = k\\
            0 &\text{otherwise}
            \end{cases} \text{ for }k = 1,\ldots, K.
    \end{equation*}
    This coding corresponds to a multinomial random variable with size 1. To 
    incorporate this information within our existing projected gamma structure
    let us consider the Dirichlet-multinomial model.  This model is established by
    assuming the categorical variable distributed multinomial with a latent 
    parameter $\pi$, and a Dirichlet prior on $\pi$. The Dirichlet-multinomial is 
    then established by integrating out the latent parameter $\pi$. 
    \begin{equation*}
    \bm{w} \sim \int_{\pi} \text{Multinom}(\bm{w}\mid\pi)\text{Dir}(\pi\mid\theta)\text{d}\pi
    \end{equation*}
    As has been established in~\cite{trubey:pg}, the Dirichlet distribution is a 
    special case of the projected gamma family, projected on the $\mathcal{L}_1$ 
    norm, and restricted such that the gamma distributions share the same rate
    parameter. Presumably we could employ the more general $\text{PG}_1$ distribution
    as the prior for $\pi$, but there is no closed form density for a 
    multinomial-$\text{PG}_1$.  Further, in the context of a kernel distribution
    for the dirichlet-process, there was no demonstrable performance advantage 
    observed to using the more general PG versus the restricted PRG distribution.
    Thus we employ $\text{PRG}_1$ as the conjugate prior for $\pi$. We can then 
    write a model for categorical data as 
    \input{./equations/model_categorical}
    Then the latent $\pi\mid \bm{w},\alpha$ can be sampled from
    \begin{equation}
        \label{eq:pi_fullcond}
        \pi \mid \bm{w},\bm{\alpha} \sim \mathcal{PRG}_1(\pi\mid\alpha + \bm{w})
    \end{equation}

\section{Mixed Models}
We make a strong assumption at this point, for establishment of the angular 
  measure, that conditional on $\theta$, the distribution of the categorical 
  variables is independent of that of the real variables.  That is, 
  $Y \in {\mathbb S}_{p}^{d-1}$ is established independent of $\bm{W}$.  This is 
  not unreasonable, as the generalized Pareto parameters estimated to transform 
  $\bm{X}$ into $\bm{Z}$ are estimated marginally, and the transformation 
  $\bm{Z}\to\bm{Y}$ is independent of $\bm{W}$.  Thus we can establish a 
  \emph{mixed} model as
  \begin{equation}
    \label{model:mixed}
    (\bm{y},\bm{w})\sim \int_{\theta}\mathcal{PG}_{p}(\bm{y}\mid\theta_1)
      \prod_{m = 1}^M\int_{\pi_m}
      \left[\text{Multinom}(\bm{w}_m\mid\pi_m)
                \mathcal{PG}_1(\pi_m\mid\theta_{2m})d\pi_{m}\right]dG(\theta),
  \end{equation}
  where $m$ indexes over categorical variables in the data. By this assumption, 
  the dependence structure (and associated inference) between $\bm{y}$ and 
  $\bm{w}$ is transferred up to the distribution of $\theta$.
  \begin{equation*}
    \label{model:mixeddp}
    \begin{aligned}
    w_i\mid \pi_i &\sim \text{Multinom}(w_i\mid \pi_i)\\
    y_i\mid\theta &\sim\mathcal{PG}_p(y_i\mid\theta^{(1)})\\
    \pi_i\mid\theta &\sim \mathcal{PG}_1(\pi_i\mid\theta^{(2)})\\
    \theta &\sim \text{DP}(\theta\mid\alpha, G_0).
    \end{aligned}
  \end{equation*}

In the general projected gamma case, the integration for $\pi$ in 
  Equation~\ref{model:cat} is not available in closed form.  Under a sample 
  based inference approach, we can augment the data by sampling $\pi$ according 
  to Eqn.~\ref{eq:pi_fullcond}, then calculate the likelihood of $\pi$ under the 
  projected Gamma distribution.  That is, for a given sample iteration,
  \begin{equation*}
    \begin{aligned}
    P(\delta_i = j\mid \ldots) \propto \begin{cases} 
    n_j^{\neg i}\mathcal{PG}_p(y_i\mid\theta_j^{(1)})
      \mathcal{PG}_1(\pi_i\mid\theta_j^{(2)}) \hspace{0.5cm}&\text{for }j = 1,\ldots,J,\\
    \frac{\eta}{m}\mathcal{PG}_p(y_i\mid\theta_j^{(1)})
      \mathcal{PG}_1(\pi_i\mid\theta_j^{(2)}) \hspace{0.5cm}&\text{for }j = J + 1,\ldots, J + m.\\
    \end{cases}
    \end{aligned}
  \end{equation*}
  As both are projected Gamma distributions, the hyper-prior structures mentioned 
  in~\cite{trubey:pg} will still be appropriate.


\subsection{Mixed Model Kernel Metrics}
\makenote{trash}
For strictly angular data, the kernel metric detailed in~\cite{trubey:pg} can 
  serve as a performant approximation to geodesic distance. in these 
  anomaly detection metrics.  However, when we include categorical data to the 
  analysis, this may require some adjustment of the kernel metric.  We elaborate 
  on a few possibilities.

In the first case, we can treat the categorical variables as merely sets of 
  additional faces.  Data then lie along the intersection between their 
  \emph{angular} face, and their \emph{categorical} face.
  \begin{equation}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
        g\left((\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right)
  \end{equation}
  \makenote{This actually requires some additional work/thought.  We would have 
  to transverse  $\sum_i(c_i - c_{i}^{\prime})^2$ faces at a minimum. I'm still 
  trying to imagine how that works--as each point would lie on the intersection 
  of some number of faces.}

In the second case, we can regard categorical variables as canonically separate 
  from the angular variables. A kernel metric in this case could be gathered as 
  the linear combination of two kernels--the angular, and one relating to the 
  categorical.  
  \begin{equation*}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \bm{c} - \bm{c}^{\prime}\rVert_2
  \end{equation*}
    
In the third case, rather than evaluating distance to $\bm{c}_i$, we can also 
  evaluate distance to $\bm{\pi}_i$, the latent categorical probability of 
  membership.  This is likely to result in a smoother decision curve.
  \begin{equation*}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \bm{c} - \bm{\pi}^{\prime}\rVert_2
  \end{equation*}

  \[
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \text{E}[\bm{\pi}] - \bm{\pi}^{\prime}\rVert_2
  \]

  \[
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \text{E}[\bm{\rho}]_1 - \bm{\pi}^{\prime}\rVert_2
  \]
  
  \begin{equation*}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
        \frac{k}{1 + k}\text{E}\left[\lVert \bm{\pi} - \bm{\pi}^{\prime}\rVert_2\right]
  \end{equation*}

  \[
    \text{E}\lVert (R\bm{y},\bm{\rho}), (R^\prime\bm{Y}^\prime,\bm{\rho}^\prime)\rVert_2
  \]


And in the fourth case, we can project $\pi$ from $\mathbb{S}_1$ to 
  $\mathbb{S}_{\infty}$, allowing use of the previously developed kernel.
  \begin{equation*}
    D\left[(\bm{y},\bm{\pi}), (\bm{y}^{\prime}, \bm{\pi}^{\prime})\right] = 
      \text{E}\left[ g\left((\bm{y},\bm{\pi}_{\infty}), 
            (\bm{y}^{\prime},\bm{\pi}_{\infty}^{\prime})\right)\right]
  \end{equation*}

The addition of categorical data to the analysis may require some adju

\begin{equation*}
  S_i^{\text{lekde}} = 
  \text{E}\left[\exp\left\lbrace-\frac{1}{2}\left(\frac{\lVert R_i\bm{v}_i - R\bm{V}\rVert}{h}\right)^2\right\rbrace\right]
\end{equation*}

\begin{equation*}
  S_i^{\text{lhkde}} = 
  \text{E}\left[ \exp\left\lbrace-\frac{1}{2}\left(\frac{d(bm{v}_i,\bm{V})}{h}\right)^2\right\rbrace\right]
\end{equation*}

Much ink has been spilled discussing the selection of the bandwidth parameter $h$.

\subsection{Mixed Model Anomaly Scores}
Let $T_{\infty}(\cdot)$ be the projection function onto the unit hypersphere.  As before, let $\bm{\rho}$ be the latent categorical probability vectors, not normalized to $\mathbb{S}_1$  Then, translating the scores in Equations~(\ref{eqn:ad_kde_h},\ref{eqn:ad_kde_e}) to the mixed data domain, we arrive at
\begin{equation}
    \label{eqn:ad_kde_mhl}
    S_i^{lhKDE} = \text{E}\left[
    \exp\left\lbrace
    -\frac{1}{2}\left(
    \frac{d(T_{\infty}(R_i\bm{v}_i,\rho_i), T_{\infty}(R\bm{V},\bm{\rho})}{h}
    \right)^2
    \right\rbrace 
    \mid \bm{v},\bm{w},\bm{v}_i, \bm{w}_i\right]
\end{equation}

\begin{equation}
    \label{eqn:ad_kde_mel}
    S_i^{leKDE} = \text{E}\left[
    \exp\left\lbrace
    -\frac{1}{2}\left(
    \frac{\lVert(R_i\bm{v}_i,\bm{\rho}_i) - (R\bm{V},\bm{\rho})\rVert_2}{h}
    \right)^2
    \right\rbrace
    \mid
    \bm{v},\bm{w},\bm{v}_i,\bm{w}_i
    \right]
\end{equation}
These scores involve a double 



\begin{equation}
    \label{eqn:ad_kde_mh}
    S_i^{hKDE} = \text{E}_{\bm{V},\bm{\rho}}\left[
    \exp\left\lbrace
    -\frac{1}{2}\left(
    \frac{d(T_{\infty}(\text{E}(R_i\bm{v}_i,\bm{\rho}_i)), T_{\infty}(R\bm{V},\bm{\rho})}{h}
    \right)^2
    \right\rbrace 
    \mid \bm{v},\bm{w},\bm{v}_i, \bm{w}_i\right]
\end{equation}

\begin{equation}
    \label{eqn:ad_kde_me}
    S_i^{eKDE} = \text{E}\left[
    \exp\left\lbrace
    -\frac{1}{2}\left(
    \frac{\lVert\text{E}(R_i\bm{v}_i,\bm{\rho}_i) - (R\bm{V},\bm{\rho})\rVert_2}{h}
    \right)^2
    \right\rbrace
    \mid
    \bm{v},\bm{w},\bm{v}_i,\bm{w}_i
    \right]
\end{equation}



% EOF