\section{Binary and Categorical Data\label{sec:categorical}}
In the previous sections we have used extreme value theory to obtain samples
    from the tail distribution of a given sample of observations. Unfortunately 
    those results can only be applied to continuous random variables.  Many 
    applications of novelty detection include both real and categorical 
    data, so here we consider an extension of the projected gamma mixture model 
    to handle categorical observations. 

Suppose $\bm{C}$ is a vector of $M$ random categorical variables.  Then $C_m$ 
    is a random categorical variable, with $K_{m} \geq 2$ categories. Regard 
    $\bm{W}_m$ as $C_m$, recoded in one-hot, or multinomial, encoding, and 
    $\bm{W}$ the concatenation of $M$ one-hot encoded categorical RV's. That is, 
    $\bm{W}$ is a binary vector of length $K = \sum_{m = 1}^M K_{m}$; 
    $\sum_{k = 1}^K W_k = M$; and every $m$ subset of $\bm{W}$ sums to 1.
    To account for over-dispersion, we consider a Dirichlet-multinomial 
    density for $\bm{W}_m$. Recall that the Dirichlet distribution is a 
    special case of projected gamma, projected onto $\mathbb{S}_1^{d-1}$, 
    with rate parameters uniformly fixed as $\beta_{\ell} = \beta = 1$ 
    by convention.  We consider a Dirichlet-multinomial distribution, $\mathcal{DM}$
    that is obtained by integrating out the latent categorical probability vector 
    from the product of Dirichlet and multinomial distributions; 
    $\mathcal{DM}(\bm{w}\mid\bm{\alpha}) = \int_{\pi}\mathcal{M}(\bm{w}\mid\pi)\mathcal{D}(\pi\mid\bm{\alpha})d\pi$.
    Recalling that a categorical random variable can be considered as a multinomial with 
    size $1$, we can further simplify the Dirichlet-multinomial to a 
    Dirichlet-categorical, reducing the computational burden. Thus,
    \begin{equation}
        \begin{aligned}
        \label{eqn:dirmultinom}
        \bm{w}_{m}\mid\bm{\alpha}_m 
        &\sim 
        \mathcal{DC}(\bm{w}_m\mid\bm{\alpha}_m) = 
        \frac{\Gamma(\sum_{\ell=1}^d \alpha_{\ell})}{\Gamma(1 + \sum_{\ell = 1}^d\alpha_{\ell})}
        \prod_{\ell = 1}^d \frac{\Gamma(w_{\ell} + \alpha_{\ell})}{\Gamma(\alpha_{\ell})}
        \end{aligned}
    \end{equation}
    We then consider a \emph{concatenated} 
    Dirichlet-categorical ($\mathcal{CDC}$) as a product of 
    Dirichlet-categorical densities.  That is, 
    $\mathcal{CDC}(\bm{w}\mid\bm{\alpha}) = 
        \prod_{m = 1}^M\mathcal{DC}(\bm{w}_m\mid\bm{\alpha}_m)$.
    Then we can define a Bayesian non-parametric categorical data model as:
    \begin{equation}
      \label{eqn:modelcat}
      \begin{aligned}
      \bm{w}_i \mid \bm{\alpha}_i &\sim 
        \mathcal{CDC}\left(\bm{w}_i\mid\bm{\alpha}_i\right)\\
      \bm{\alpha_i} &\sim G\\
      G &\sim \mathcal{PY}\left(d, \eta, G_0\right)\\
      \end{aligned}
      ~\hspace{1cm}
      \begin{aligned}
      G_0 &= \mathcal{LN}\left(\bm{\alpha}\mid\bm{\mu},\Sigma\right)\\
      \mu &\sim \mathcal{N}\left(\bm{0},\bm{1}\right)\\
      \Sigma &\sim \mathcal{IW}\left(\nu, \Psi\right).
      \end{aligned}
    \end{equation}
    Note that there  exists a strong negative covariance between categories 
    within a categorical variable.  To account for this in our proposed prior, the 
    parameter $\Psi$ is chosen as a block diagonal matrix, with each $m$ block 
    corresponding to the $m$th categorical variable.  Setting the value of the diagonal 
    to $\psi_0$, the off-diagonals within the $m$ block are set to $-\psi_0 d_m^{-2}$ 
    where $d_m$ is the number of categories in the $m$th categorical variable.
    This value corresponds to the covariance of a categorical variable where all 
    category probabilities are equal. In addition to the proposed log-normal model,
    we investigated using a product of gammas as the centering distribution in
    Equation~(\ref{eqn:modelcat}), but we observed that this choice induces
    numerical instability.  We observed that the log-normal distribution, with
    its squared exponential tails and ability to account for negative covariance
    within the prior, provided stable model fitting.

\subsection{Anomaly Detection Methods for Categorical Data\label{subsec:catscores}}
Anomaly scores analogous to the ones proposed in Section \ref{sec:novelty} can be 
obtained for categorical variables by transforming the latent variables that define
a Dirichlet-Multinomial distribution on $\mathcal{S}_1^{d-1}$ to $\mathcal{S}_\infty^{d-1}$. 
We start by considering the cluster identifiers. Extrapolating Equation~(\ref{eqn:clusterid})
    to the categorical model, cluster identifiers $\varphi_i$ are sampled with probabilities
    \begin{equation}
        \label{eqn:latentposterior}
        \text{Pr}[\varphi_i = j\mid \bm{\alpha}, \bm{p}, \bm{w}_i] = \frac{p_j\,\mathcal{CDC}\left(\bm{w}_i\mid\bm{\alpha}_j\right)}{\sum_{k = 1}^J\,p_k\,\mathcal{CDC}\left(\bm{w}_i\mid\bm{\alpha}_k\right)}
            \;\;\;\text{ for }j = 1, \ldots, J.
    \end{equation}
    For a given sample from the posterior for $\bm{\alpha}$,
    first we sample $\varphi_i$, then let $\bm{\alpha}_i = \bm{\alpha}_{\varphi_i}$, and
    sample
    \begin{equation}
        \label{eqn:postpredrho}
        \bm{\varrho}_i\mid\bm{\alpha}_i \sim 
        \prod_{\ell = 1}^d\mathcal{G}\left(\varrho_{i\ell}\mid\alpha_{i\ell}, 1\right).
    \end{equation}
    These are the latent variables that provide the core structure to the categorical
    data model. In fact, the component probability vectors for the concatenated multinomial
    are obtained by transforming $\bm{\varrho}_i$ onto $\prod_{m = 1}^M\mathbb{S}_1^{{d_m}-1}$
    to produce $\bm{\pi}_i = \prod_{m=1}^{M}T_1(\bm{\varrho}_{im})$. Anomaly scores
    analogous to the ones proposed in the continuous case can then be obtained by
    letting $\bm{\nu}_i = T_{\infty}(\bm{\varrho}_i)$, the transformation of $\bm{\varrho}_i$ 
    onto $\mathbb{S}_{\infty}^{d-1}$.
\st{To establish an anomaly detection algorithm using the data model described in 
    Equation, we consider a method using logic similar to 
    that described in Section.  We desire a transformation that 
    can be projected onto the unit sphere $\mathbb{S}_{\infty}^{d-1}$.  To that 
    end, rather than using $\bm{w}_i$ directly, we use data augmentation
    described in Equation~ to generate $\bm{\varrho}_i$, 
    the latent unnormalized posterior class probabilities for observation $i$, 
    sampled given $\bm{w}_i$.}  It is important to notice that distance metrics 
    between projections of $\bm{\varrho}_i$ and  replicates of $\bm{\varrho}^*$ 
    from the posterior predictive distribution is straightforward. This provides
    a distinct advantage to the approach based on the distance between the 
    directly observed values $\bm{w}_i$ and samples of $\bm{W}$, obtained 
    from the corresponding posterior predictive distribution~\citep{Alamuri2014}. 
    
    We develop four methods based
    on applications of the KNN and KDE metrics previously described.
    Making an abuse of notation for simplicity of presentation, let 
    $\tilde{\text{E}}\left[\bm{\nu}_i\right] := T_{\infty}(\expect{\bm{\nu}_i\mid\bm{w}_i})$, 
    % $\add{\bf What is $\bm{\nu}_i$? } % I think comment no longer relevant; 
    % $\nu$ introduced at end of previous paragraph.
    the projection of the expectation of $\bm{\nu}_i$ back onto 
    $\mathbb{S}_{\infty}^{d-1}$.  Evaluating this expectation by
    Monte Carlo approximation is equivalent calculating the spherical mean 
    \citep{mardia1999}, which takes the arithmetic mean of observations in 
    Cartesian coordinates, then projects back onto the sphere.

The hypercube KNN \emph{(h$k$nn)} metric applied to the latent projected 
    $\mathbb{S}_{\infty}^{d-1}$ space uses the negative definite kernel metric 
    previously established to estimate distance between $\tilde{\text{E}}[\bm{\nu}_i]$
    and $\bm{\nu}^*$.  This score takes the form:
    \begin{equation}
      \label{score:cat_hknn}
      \score_{i,\bm{\nu}}^{\text{h$k$nn}} = 
      \frac{N\,\pi^{\frac{d-1}{2}}}{k\,\Gamma\left(\frac{d-1}{2} + 1\right)}
        \;D_{k}\left(\tilde{\text{E}}[\bm{\nu}_i]\right)^{d-1}
    \end{equation}
    where $D_{k}\left(\tilde{\text{E}}[\bm{\nu}_i]\right)$ measures the 
    distance from $\tilde{\text{E}}[\bm{\nu}_i]$ to the $k$th nearest replicate from 
    a sample from the posterior predictive distribution for $\bm{\nu}^*$.  
    This projection places all the class probabilities within the same sphere and 
    subject to the same distance measure.  Note here we are first taking the 
    expectation of $\bm{\nu}_i$, then the expectation of the kernel metric raised 
    to the $d-1$ power.

The \emph{hkde} score applied to the categorical space operates in much the 
    same way.  We compute $\tilde{\text{E}}[\bm{\nu}_i]$, and employ the same 
    kernel metric to compute distance from a sample from the posterior predictive 
    distribution.  From there, however, we use kernel density estimation
    to compute local density for observation $i$.  The score is then
    \begin{equation}
        \label{score:cat_hkde}
        \score_{i,\bm{\nu}}^{\text{hkde}} = \text{E}_{\bm{\nu}^*}\left[
            \exp\left\lbrace
            -\frac{1}{2}\left(
            \frac{g(\tilde{\text{E}}[\bm{\nu}_i], \bm{\nu}^*)}{\hat{h}}
            \right)^2
            \right\rbrace
            \right]^{-1} \approx \left[\frac{1}{K}\sum_{k = 1}^{K}
                \exp\left\lbrace-\frac{1}{2}\left(
                \frac{g(\tilde{\text{E}}[\bm{\nu}_i],\bm{\nu}_k^*)}{\hat{h}}
                \right)^2\right\rbrace\right]^{-1}
    \end{equation}
We use the same previously described approach to choose $h$.  
    An exploration of manually tuning $h$ did not consistently outperform 
    the rule of thumb estimator.

Notice that the \emph{hkde} score depends on two expectations that are computed
in sequence. A variant of the score is obtained by computing the 
    expectations jointly:
    \begin{equation}
        \label{score:cat_lhkde}
        \score_{i,\bm{\nu}}^{\text{lhkde}} = \text{E}_{\bm{\nu},\bm{\nu}_i}\left[
        \exp\left\lbrace
        -\frac{1}{2}\left(
        \frac{g(\bm{\nu}_i, \bm{\nu}^*)}{\hat{h}}
        \right)^2
        \right\rbrace
        \right]^{-1} \approx \left[\frac{1}{K_{\bm{\nu}_i}K_{\bm{\nu}^*}}
            \sum_{j = 1}^{K_{\bm{\nu}_i}}\sum_{k = 1}^{K_{\bm{\nu}^*}}
            \exp\left\lbrace-\frac{1}{2}
                \left(\frac{g(\bm{\nu}_{i,j},\bm{\nu}_{k}^*)}{\hat{h}}\right)^2
                \right\rbrace\right]^{-1}
    \end{equation}
    Computing this for a given sample is more expensive 
    than \emph{hkde} due to the double sum.  However, plugging in an estimate of
    $\expect{\bm{v}_{\bm{w}_i}}$ removes a significant degree of uncertainty around the 
    distribution of $\bm{v}_{\bm{w}_i}$, which may be relevant.

If, instead of projecting the unnormalized probability vectors onto a unified 
    hypersphere $\mathbb{S}_{\infty}^{d-1}$, we normalize each $m$-component 
    onto its associated simplex, $\mathbb{S}_1^{d_m - 1}$.  Using Manhattan distance on the simplex, we 
    obtain the latent simplex KDE \emph{(lskde)}.
    \begin{equation}
        \label{score:cat_lskde}
        \score_{i,\bm{\pi}}^{\text{lskde}} = \text{E}_{\bm{\pi}_i,\bm{\pi}^*}\left[\exp\left\lbrace
        -\frac{1}{2}\left(
        \frac{\lVert \bm{\pi}_i - \bm{\pi}^*\rVert_1}{\hat{h}}
        \right)^2
        \right\rbrace
        \right]
        \approx \left[
            \frac{1}{K}\sum_{k = 1}^K\exp\left\lbrace
            -\frac{1}{2}\left(\frac{\lVert \bm{\pi}_i - \bm{\pi}_k^*\rVert}{\hat{h}}\right)^2
            \right\rbrace
        \right]^{-1}
    \end{equation}
    Using the normalized latent class probabilities offers the advantage of
    numerical stability: diverging estimates of $\varrho$ are isolated to the
    relevant $m$-component.

\begin{algorithm}[htb]
    \caption{Workflow for anomaly detection for categorical data}\label{alg:adcat}
    % \begin{enumerate}[nolistsep]
        % \item
    \begin{algorithmic}[1]
        \State Take $\bm{w}$ as the conatenation of $m$ multinomial-encoded categorical variables.
        \State Take $d := \sum_{m = 1}^M d_m$ as the dimensionality of the process
        \State Fit $\bm{w}\sim\mathcal{PYCDC}$ from Equation~(\ref{eqn:modelcat})
        \State From $\bm{\alpha}\mid\bm{w}$, sample $\bm{\varrho}_k^*\mid\bm{\alpha}\sim \prod_{\ell}\mathcal{G}(\alpha_{\ell})$ for $k = 1,\ldots,K_{\nu}$; then $\bm{\nu}^* = T_{\infty}(\bm{\varrho}^*)$
        \State From $\bm{\alpha}_i\mid\bm{w}_i$ sampled as per Equations~(\ref{eqn:latentposterior}-\ref{eqn:postpredrho})
        
        sample $\bm{\varrho}_{ik}\mid\bm{\alpha}_{i}\sim\prod_{\ell}\mathcal{G}(\alpha_{\ell})$ for $k = 1,\ldots,K_{\nu_i}$
        \State Take $\bm{v}_{ik} = T_{\infty}(\bm{\varrho}_{ik})$;\hspace{0.15cm}
            $\bm{\pi}_{ik} = \prod_{m = 1}^MT_{1}(\bm{\varrho}_{ikm})$
        \State Take $\score_{i\bm{v}}$ as per Equations~(\ref{score:cat_hknn}--\ref{score:cat_lskde})
            %\ref{score:cat_hknn, score:cat_hkde, score:cat_lhkde, score:cat_lskde})
    \end{algorithmic}
\end{algorithm}




% EOF