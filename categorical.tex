\section{Binary and Categorical Data}
Many applications for which novelty detection methods might be applied will 
    feature both real and categorical data.  \makenote{rewrite} The most apparent 
    shortcoming of the projected Gamma model developed in~\cite{trubey:pg} is
    its inability to handle binary or categorical data.  To ameliorate this 
    shortcoming, we propose the following.

Let $C_{im}$ be a random categorical variable, with $K_m \geq 2$ categories.
  We recode $C$ in one-hot encoding, corresponding to a multinomial random
  variable with size 1.  To incorporate this information into our existing
  projected gamma structure, we consider a Dirichlet-multinomial density,
  where 
  \begin{equation*}
    \bm{w}_{im}\mid\bm{\alpha} 
    \sim 
    \int_{\pi_{im}} 
    \text{Multinom}(\bm{w}_{im}\mid\pi_{im})
    \text{Dir}(\pi\mid\bm{\alpha})\text{d}\pi_{im}.
  \end{equation*}
  Recall that the Dirichlet distribution is a special case of the projected gamma,
  where the rate parameters are uniformly fixed $\beta_{\ell} = \beta$.  As seen
  in \cite{trubey:pg}, in the context of a dirichlet process mixture, there was
  not a demonstrable performance advantage 
  
  
  
  Consider,
  then, a \emph{concatenated} dirichlet-multinomial (CDM) as a product of
  Dirichlet-multinomials. 


\makenote{This is way too long.  rewrite.}
Let $C$ be a random categorical variable, with $K \geq 2$ categories.  We 
    re-code $C$ into the random vector $\bm{W}$ with length $K$, taking the form
    \begin{equation*}
        W_k = \begin{cases}
            1 &\text{if }C = k\\
            0 &\text{otherwise}
            \end{cases} \text{ for }k = 1,\ldots, K.
    \end{equation*}
    This coding corresponds to a multinomial random variable with size 1. To 
    incorporate this information within our existing projected gamma structure
    let us consider the Dirichlet-multinomial model.  This model is established by
    assuming the categorical variable distributed multinomial with a latent 
    parameter $\pi$, and a Dirichlet prior on $\pi$. The Dirichlet-multinomial is 
    then established by integrating out the latent parameter $\pi$. 
    \begin{equation*}
    \bm{w} \sim \int_{\pi} \text{Multinom}(\bm{w}\mid\pi)\text{Dir}(\pi\mid\theta)\text{d}\pi
    \end{equation*}
    As has been established in~\cite{trubey:pg}, the Dirichlet distribution is a 
    special case of the projected gamma family, projected on the $\mathcal{L}_1$ 
    norm, and restricted such that the gamma distributions share the same rate
    parameter. Presumably we could employ the more general $\text{PG}_1$ distribution
    as the prior for $\pi$, but there is no closed form density for a 
    multinomial-$\text{PG}_1$.  Further, in the context of a kernel distribution
    for the dirichlet-process, there was no demonstrable performance advantage 
    observed to using the more general PG versus the restricted PRG distribution.
    Thus we employ $\text{PRG}_1$ as the conjugate prior for $\pi$. We can then 
    write a model for categorical data as 
    \input{./equations/model_categorical}
    Then the latent $\pi\mid \bm{w},\alpha$ can be sampled from
    \begin{equation}
        \label{eq:pi_fullcond}
        \pi \mid \bm{w},\bm{\alpha} \sim \mathcal{PRG}_1(\pi\mid\alpha + \bm{w})
    \end{equation}

\section{Mixed Models}
We make a strong assumption at this point, for establishment of the angular 
  measure, that conditional on $\theta$, the distribution of the categorical 
  variables is independent of that of the real variables.  That is, 
  $Y \in {\mathbb S}_{p}^{d-1}$ is established independent of $\bm{W}$.  This is 
  not unreasonable, as the generalized Pareto parameters estimated to transform 
  $\bm{X}$ into $\bm{Z}$ are estimated marginally, and the transformation 
  $\bm{Z}\to\bm{Y}$ is independent of $\bm{W}$.  Thus we can establish a 
  \emph{mixed} model as
  \begin{equation}
    \label{model:mixed}
    (\bm{y},\bm{w})\sim \int_{\theta}\mathcal{PG}_{p}(\bm{y}\mid\theta_1)
      \prod_{m = 1}^M\int_{\pi_m}
      \left[\text{Multinom}(\bm{w}_m\mid\pi_m)
                \mathcal{PG}_1(\pi_m\mid\theta_{2m})d\pi_{m}\right]dG(\theta),
  \end{equation}
  where $m$ indexes over categorical variables in the data. By this assumption, 
  the dependence structure (and associated inference) between $\bm{y}$ and 
  $\bm{w}$ is transferred up to the distribution of $\theta$.
  \begin{equation*}
    \label{model:mixeddp}
    \begin{aligned}
    w_i\mid \pi_i &\sim \text{Multinom}(w_i\mid \pi_i)\\
    y_i\mid\theta &\sim\mathcal{PG}_p(y_i\mid\theta^{(1)})\\
    \pi_i\mid\theta &\sim \mathcal{PG}_1(\pi_i\mid\theta^{(2)})\\
    \theta &\sim \text{DP}(\theta\mid\alpha, G_0).
    \end{aligned}
  \end{equation*}

In the general projected gamma case, the integration for $\pi$ in 
  Equation~\ref{model:cat} is not available in closed form.  Under a sample 
  based inference approach, we can augment the data by sampling $\pi$ according 
  to Eqn.~\ref{eq:pi_fullcond}, then calculate the likelihood of $\pi$ under the 
  projected Gamma distribution.  That is, for a given sample iteration,
  \begin{equation*}
    \begin{aligned}
    P(\delta_i = j\mid \ldots) \propto \begin{cases} 
    n_j^{\neg i}\mathcal{PG}_p(y_i\mid\theta_j^{(1)})
      \mathcal{PG}_1(\pi_i\mid\theta_j^{(2)}) \hspace{0.5cm}&\text{for }j = 1,\ldots,J,\\
    \frac{\eta}{m}\mathcal{PG}_p(y_i\mid\theta_j^{(1)})
      \mathcal{PG}_1(\pi_i\mid\theta_j^{(2)}) \hspace{0.5cm}&\text{for }j = J + 1,\ldots, J + m.\\
    \end{cases}
    \end{aligned}
  \end{equation*}
  As both are projected Gamma distributions, the hyper-prior structures mentioned 
  in~\cite{trubey:pg} will still be appropriate.


\subsection{Mixed Model Kernel Metrics}
\makenote{This subsection is junk.}
For strictly angular data, the kernel metric detailed in~\cite{trubey:pg} can 
  serve as a performant analogue for \emph{distance} measurement in these 
  anomaly detection metrics.  However, when we include categorical data to the 
  analysis, this may require some adjustment of the kernel metric.  We elaborate 
  on a few possibilities.

In the first case, we can treat the categorical variables as merely sets of 
  additional faces.  Data then lie along the intersection between their 
  \emph{angular} face, and their \emph{categorical} face.
  \begin{equation}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
        g\left((\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right)
  \end{equation}
  \makenote{This actually requires some additional work/thought.  We would have 
  to transverse  $\sum_i(c_i - c_{i}^{\prime})^2$ faces at a minimum. I'm still 
  trying to imagine how that works--as each point would lie on the intersection 
  of some number of faces.}

In the second case, we can regard categorical variables as canonically separate 
  from the angular variables. A kernel metric in this case could be gathered as 
  the linear combination of two kernels--the angular, and one relating to the 
  categorical.  
  \begin{equation*}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \bm{c} - \bm{c}^{\prime}\rVert_2
  \end{equation*}
    
In the third case, rather than evaluating distance to $\bm{c}_i$, we can also 
  evaluate distance to $\bm{\pi}_i$, the latent categorical probability of 
  membership.  This is likely to result in a smoother decision curve.
  \begin{equation*}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \bm{c} - \bm{\pi}^{\prime}\rVert_2
  \end{equation*}

  \[
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \text{E}[\bm{\pi}] - \bm{\pi}^{\prime}\rVert_2
  \]

  \[
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
          \frac{k}{1 + k}\lVert \text{E}[\bm{\rho}]_1 - \bm{\pi}^{\prime}\rVert_2
  \]
  
  \begin{equation*}
    D\left[(\bm{y},\bm{c}), (\bm{y}^{\prime}, \bm{c}^{\prime})\right] = 
      g(\bm{y},\bm{y}^{\prime}) + 
        \frac{k}{1 + k}\text{E}\left[\lVert \bm{\pi} - \bm{\pi}^{\prime}\rVert_2\right]
  \end{equation*}

  \[
    \text{E}\lVert (R\bm{y},\bm{\rho}), (R^\prime\bm{Y}^\prime,\bm{\rho}^\prime)\rVert_2
  \]


And in the fourth case, we can project $\pi$ from $\mathbb{S}_1$ to 
  $\mathbb{S}_{\infty}$, allowing use of the previously developed kernel.
  \begin{equation*}
    D\left[(\bm{y},\bm{\pi}), (\bm{y}^{\prime}, \bm{\pi}^{\prime})\right] = 
      \text{E}\left[ g\left((\bm{y},\bm{\pi}_{\infty}), 
            (\bm{y}^{\prime},\bm{\pi}_{\infty}^{\prime})\right)\right]
  \end{equation*}

The addition of categorical data to the analysis may require some adju

\subsection{}

\begin{equation*}
  S_i^{\text{lhkde}} = \text{E}
\end{equation*}



% EOF