\subsection{Mixed Models}
To obtain a joint model for the density of a vector with mixed components
    we consider a product kernel, then mix over the parameters that define 
    both kernels in order to capture the dependence between components. Thus,
    \begin{equation}
        \label{model:mixed}
        (\bm{y},\bm{w})\sim \int_{\bm{\alpha}}\mathcal{PG}_{p}
            (\bm{y}\mid\bm{\alpha}_{\bm{y}}, \bm{1})
        \;\mathcal{CDM}(\bm{w}\mid\bm{\alpha}_{\bm{w}})\;dG(\bm{\alpha})
    \end{equation}
  with the distribution of $\bm{\alpha}=(\bm{\alpha_\bm{y},\bm{\alpha_\bm{w}})}$ as
    defined in Equation~\ref{eqn:modelcat}. The dimensions are, respectively, 
    $d_{\bm{y}}$ and $d_{\bm{w}}$. Note that for 
    the projected gamma distribution, we restrict the rate parameters 
    to $\beta_{\ell} := 1$.  Also note that for the mixed model, the hyperparameter 
    for the covariance matrix $\Sigma_{\bm{\alpha}}$ is taken as a blocked diagonal 
    matrix, with the block corresponding to the angular component being a 
    diagonal matrix.

\subsection{Mixed Model Anomaly Scores\label{sec:mixedscores}}
Let $d = d_{\bm{y}} + d_{\bm{w}}$ be the total number of dimensions.  
    Then, for the mixed model, let 
    $\bm{\nu}_i = T_{\infty}(R_i\bm{y}_i, \bm{\varrho}_{i\bm{w}})$, and 
    $\bm{\nu} = T_{\infty}(\bm{\varrho})$.  The \emph{hknn} score can be adapted 
    to the mixed model by re-projecting the angular data and the latent 
    categorical component into the same sphere. This requires moving $\bm{y}_i$ 
    back to $\mathbb{R}_+^{d_{\bm{y}}}$, by multiplying by the radial component
    generated according to
    \begin{equation}
        \label{eqn:angularradial}
        R_i\mid\bm{\alpha} \sim \mathcal{G}
        \left(R_i\given\sum_{\ell = 1}^{d_{\bm{y}}}\alpha_{\ell}, 1\right).
    \end{equation}
    Then $\bm{\nu}_i = T_{\infty}(R_i\bm{y}_i, \bm{\varrho}_{i\bm{w}})$ is the 
    latent projection of both the real component and categorical component into 
    the same sphere. Also, let $\bm{\nu} = T_{\infty}(\bm{\varrho})$ be the 
    generic $\bm{\nu}$ not specifically dependent on observation $i$.  To obtain the 
    corresponding anomaly scores we can proceed by using the expression in equations
    (\ref{score:cat_hknn}), (\ref{score:cat_hkde}) and (\ref{score:cat_lhkde}) 

All three scores seek a unifying approach for all data, projecting onto a the same 
    sphere, and calculating a 
    consistent distance metric.  An alternative is to, instead, evaluate distances 
    between angular data their own space, and, separately, latent posterior class 
    probabilities in their own space, with the appropriate distance metric for 
    each.  In effect, this approach combines \emph{hkde} from
    the angular component and \emph{lskde} from the categorical component yielding:
    \begin{equation}
    \label{eqn:lmkdecat}
    \begin{aligned}
    \score_{i,\bm{v}}^{lmkde} &= \text{E}_{\bm{v}^*,\bm{\pi}^*,\bm{\pi}_i}\left[
        \exp
        \left\lbrace 
        -\frac{1}{2}
        \left(
        \frac{d\left(\bm{v}_i, \bm{v}^*\right)}{\hat{h}_{\bm{v}^*}}
        \right)^2
        -\frac{1}{2}
        \left(
        \frac{\lVert\bm{\pi}_i - \bm{\pi}^*\rVert_1}{\hat{h}_{\bm{\pi}^*}}
        \right)^2
        \right\rbrace
        \right]^{-1}\\
        &\approx \left[\frac{1}{K_{\bm{\pi}^*}K_{\bm{\pi}_i}}
            \sum_{j = 1}^{K_{\bm{\pi}_i}}\sum_{k=1}^{K_{\bm{\pi}^*}}
            \exp\left\lbrace-\frac{1}{2}
            \left(\frac{d(\bm{v}_i,\bm{v}_k^*)}{\hat{h}_{\bm{v}^*}}\right)^2
            -\frac{1}{2}\left(
            \frac{\lVert\bm{\pi}_{ij} - \bm{\pi}_k^*\rVert_1}{\hat{h}_{\bm{\pi}^*}}
            \right)^2
            \right\rbrace\right]^{-1}
    \end{aligned}
    \end{equation}
    This choice to evaluate each component within its own space presents some 
    loss of information as to the dependence structure between $\bm{y}$ and 
    $\bm{w}$ within the score.  We will explore to what extent that loss of 
    information is relevant.

\begin{algorithm}[htb]
    \caption{Workflow for anomaly detection for \emph{mixed} data}\label{alg:admixed}
    \begin{algorithmic}[1]
        \State Take $r_i$, $\bm{y}_i$ according to Algorithm~(\ref{alg:workflow}); $\bm{w}_i$ as in Algorithm~\ref{alg:adcat}.
        \State Fit $(\bm{y},\bm{w})$ using mixed model from Equation~(\ref{model:mixed})
        \State From $\bm{\alpha}\mid\bm{y},\bm{w}$, sample
            $\bm{\varrho}_k^{*}\mid\bm{\alpha}\sim\prod_{\ell}\mathcal{G}(\alpha_{\ell})$ for $k = 1,\ldots,K$
        \If{$\score_{i,\bm{v}}$ is \emph{hknn}, \emph{hkde}, or \emph{lhkde}}
            \State From $\bm{\alpha}\mid\bm{y}_i,\bm{w}_i$:
                sample $R_i$ according to Equation~(\ref{eqn:angularradial}), 
                $\bm{\varrho}_{iw}$ similar to Algorithm~\ref{alg:adcat}.
            \State Take $\bm{\nu}_i = T_{infty}(R_i\bm{y}_i,\bm{\varrho}_{iw})$;      
                $\bm{\nu}^* = T_{\infty}(\bm{\varrho}^*)$.
            \State Apply Score function.
        \ElsIf{$\score_{i,\bm{v}}$ is \emph{lmkde}}
            \State From $\bm{\alpha}\mid\bm{y}_i,\bm{w}_i$, 
                sample $\bm{\varrho}_{iw}$ similar to Algorithm~\ref{alg:adcat}.
            \State Take $\bm{\pi}_i = \prod_{m = 1}^M T_{1}(\bm{\varrho}_{im})$;    
                $\bm{\pi}^* = \prod_{m = 1}^{M} T_1(\bm{\varrho}^*)$
            \State Apply Score function.
        \EndIf
    \end{algorithmic}
\end{algorithm}

\subsection{Relaxing the assumption of independence\label{subsec:rank}}
A valid critique of the model presented thus far is that in order to justify 
    modelling the radial component of $\bm{Z}$ as independent to its angular 
    component---the fundamental result of the multivariate extreme value theory 
    presented---it is necessary to subset data to those observations $\bm{X}$ 
    which exceeded a large threshold in at least one dimension.  For some 
    applications, this represents a very powerful data reduction with little loss of 
    information pertaining to anomalies, as anomalies tend to be in the tails 
    (see, for  example, Table~\ref{tab:data}).  For other applications, this 
    data reduction represents a significant loss of information about possible
    anomalies not corresponding to the tails.  For this second group,
    one available avenue is to relax the assumption of independence between the 
    angular and radial components.

Let $z_{i\ell} = 1/(1 - \hat{F}(x_{i\ell}))$ be the 
    \emph{rank-transformation} to the standard Pareto scale.  The lower range 
    of this transformation is bounded at 1. For data transformed in this manner, 
    let $r_i = \lVert \bm{z}_i\rVert_{\infty}$ be the radial component, 
    $\bm{v}_i = \bm{z}_i/r_i$ the angular component of $\bm{z}_i$, and 
    $\bm{y}_i$ its projection onto $\mathbb{S}_p^{d-1}$.  
    As no thresholding is performed we can no longer make the assumption that 
    angles are independent of radius.  Instead, we can include the radius 
    within a joint model.  As the radius is on the range $[1,\infty)$, 
    we use the Pareto density, with shape parameter $\alpha_r$ as our choice 
    of kernel.  
    \begin{equation}
        \label{model:rank}
        (\bm{y}_i,\bm{w}_i,r_i) \sim \int_{\bm{\alpha}}
            \mathcal{PG}_p(\bm{y}_i\mid\bm{\alpha}_{\bm{y}}, \bm{1})\;
            \mathcal{CDM}(\bm{w}_i\mid\bm{\alpha}_{\bm{w}})\;
            \mathcal{P}(r_i\mid\alpha_r)\;
            \text{d}G(\bm{\alpha})
    \end{equation}
    As $\alpha_r > 0$, we augment the kernel parameters to $\bm{\alpha}= 
    (\bm{\alpha}_{\bm{y}}, \bm{\alpha}_{\bm{w}}, \alpha_r)$, and use a joint log-normal
    as the center of the random measure prior for $G$. The scores developed previously in 
    Section~\ref{sec:mixedscores} remain applicable.

% EOF