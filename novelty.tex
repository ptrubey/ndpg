\section{Novelty Detection Methods\label{sec:novelty}}
As previously stated, a novelty detection algorithm produces an anomaly score 
    which provides a ranked ordering of which observations are likely to be 
    anomalous, with higher scores indicating more likely anomalous. Building on 
    the notion that anomalies occur in areas of low density, a general score 
    might establish for observation $x_i$, 
    \[S_i = \text{E}\left[f_{x}(x_i\mid\mathcal{D})^{-1}\right]\]
    where $\mathcal{D}$ is the observed data.  That is, the reciprocal of the 
    posterior predictive density at observation $x_i$.

With independence established between the angular and radial components of an
    extreme observation, we can consider sub-scores established under radial 
    and angular components independently.  That is,
    \[
        S_i = S_i^{(r)}\times S_i^{(\bm{v})} = 
            \text{E}\left[f_r(r_i\mid\mathcal{D})^{-1}\right]\times
            \text{E}\left[f_v(\bm{v}_i\mid\mathcal{D})^{-1}\right]
    \]
    As we have established, $r_i$ follows a standard Pareto distribution, so its
    density $f_r(r_i) = r_i^{-2}$.  Estimating the angular density, 
    $f_v(\bm{v}_i)$, is more complicated.  As we have previously established in
    Section~\ref{sec:evt}, density estimation on the surface of 
    ${\mathbb S}_{\infty}^{d-1}$ is not an easy problem. To establish a method
    of inferring such a distribution, we instead projected the data from 
    $\mathbb{S}_{\infty}^{d-1}$ onto $\mathbb{S}_p^{d-1}$ for a large but finite
    $p$.  This makes estimation of distributional parameters possible, but a
    calculated density for a given observation under a finite $p > 1$ depends on
    which dimension $\ell \in \lbrace 1,\ldots,d\rbrace$ was used as the summary
    variable.  Calculated density, and thus rank ordering of anomaly scores,
    could change depending on which dimension was used.  This is not ideal.
    Further, for an observation near 0 evaluated on a gamma-related distribution 
    with a shape parameter $\alpha$ near 0 creates an evaluated density that 
    tends towards infinity.  This instability would play havoc on directly 
    calculated density-based scores.

We sidestep these problems of angular density evaluation by establishing a 
    non-parametric angular density estimator, using as input a sample from the 
    posterior predictive distribution of the model established in 
    Section~\ref{sec:evt}. By fitting an angular model to the data, we can 
    generate an arbitrarily large sample with which to establish the 
    non-parametric density estimator.  Here, we consider two well-established
    methods: $k$-nearest neighbors, or \emph{$k$NN}, and 
    kernel density estimation, or \emph{KDE}.  For both of these methods we make
    use of pairwise distances between observations from the dataset, and
    replicates from a posterior predictive sample.  

As described in \cite{trubey:pg}, \emph{distance} on $\mathbb{S}_{\infty}^{d-1}$ 
    is computationally expensive to evaluate.  Instead, they propose an estimate
    of distance that is computationally cheap to evaluate, bearing a cost
    equivalent to Euclidean norm.  
    % \makenote{option 1}
    For observations on different faces of 
    $\mathbb{S}_{\infty}^{d-1}$, a performant analogue to distance can be
    evaluated by rotating the face of the second observation along the axis of
    the first observation, and computing the Euclidean norm between the first
    observation and the transformed second observation.  
    % \makenote{option 2}
    % Let
    % ${\mathbb C}_{\ell}^{d-1} = \lbrace \bm{x} : 
    %     \bm{x} \in {\mathbb S}_{\infty}^{d-1}, x_{\ell} = 1\rbrace$
    % comprise the $\ell$th face of $\mathbb{S}_{\infty}^{d-1}$.  For two points
    % on the same face, Euclidean distance corresponds to the shortest possible
    % path between those points.  For points 
    % $\bm{a} \in \mathbb{C}_{\ell}^{d-1},\;\bm{b}\in\mathbb{C}_{\jmath}^{d-1}$,
    % we can rotate $\mathbb{C}_{\jmath}^{d-1}$ into the same hyperplane as 
    % $\mathbb{C}_{\ell}^{d-1}$ as
    % \begin{equation}
    %     \bm{b}^{\prime}_i = P_{\jmath\ell}(\bm{b}) = 
    %     \begin{cases}
    %         b_{i} &\text{for }i\neq \jmath,\ell\\
    %         1 &\text{for }i = \ell\\
    %         2 - b_{\ell} &\text{for }i = \jmath
    %     \end{cases},\;\;\;\;
    %     g(\bm{a},\bm{b}) = \pnorm{\bm{a} - \bm{b}^{\prime}}{2}
    % \end{equation}
    % After transformation, the Euclidean norm between $\bm{a}$ and transformed
    % provides a 
    Indeed, this metric can
    be considered an upper bound on distance on $\mathbb{S}_{\infty}^{d-1}$
    between the two observations.

% \add{\bf This whole section needs to rewritten. First we have to connect to 
%     the previous section. As a result of what the methods presented in the 
%     previous section we have a sample from the angular density that 
%     characterizes the extreme behavior of our observations. We have samples, 
%     but we have no way to evaluate it. At this point you provide information 
%     about estimated of the density, and then you have to mention that a 
%     measure of distance on ${\mathbb S}_{\infty}^{d-1}$, and you refer to the 
%     previous paper.}

% \makenote{A reviewer might ask about $p=1$, and using the direct density 
%   calculated thereof.  We should add that fitted gamma densities are inherently 
%   unstable and the spiking behavior near axes results in \emph{extremely} 
%   non-normal behavior.  True density metrics based on projected gamma (at any $p$) 
%   are not going to behave properly and are undesireable.}

\subsection{$k$-Nearest Neighbors Density estimation}
We establish a local posterior predictive density under the $k$NN estimator 
    \citep{mack1979} on $\mathbb{S}_{\infty}^{d-1}$ by assuming a locally 
    uniform density within a $d-1$--dimensional ball $\mathbb{B}$, centered on 
    observation $i$, with radius $D_{k}(\bm{v}_i)$ calculated as the aforementioned
    distance analogue \makenote{would prefer a ref to callback to} to 
    the $k$th nearest neighbor to observation $i$ in a sample from the 
    posterior predictive distribution. The volume of the ball is calculated as
    \begin{equation}
        \label{eq:vol_sphere}
        \text{Vol}(\mathbb{B}_{d-1}^k) =
        \frac{\pi^{\frac{d-1}{2}}D_{k}(\bm{v}_i)^{d-1}}{
            \Gamma\left(\frac{d-1}{2} + 1\right)}.
    \end{equation}
    A density is thus established as 
    $f_{x}^{(k\text{NN})}(x_i\mid X) \approx 
        \frac{k}{N}\left(\text{Vol}(\mathbb{B}_{d-1}^k)\right)^{-1}$
    where $N$ is the total number of replicates of from the posterior predictive
    distribution.  Taking the reci[rocal of the estimated angular density, 
    the angular score under the $k$NN estimator is then
    \begin{equation}
        \label{eq:ad_knn_h}
        S_i^{(\bm{v})} = \frac{r_i^{2}N}{k}
            \frac{\pi^{\frac{d-1}{2}}D_{k}(\bm{v}_i)^{d-1}}{
            \Gamma\left(\frac{d-1}{2} - 1\right)}
    \end{equation}
    In our experience, using a large posterior predictive sample, 
        \makenote{this should be quantified}
    the resulting odering of scores was relatively robust to choice of $k$, for
    values between 2 and 10.  We used $k = 5$ in our performance analysis.
% When implementing this analysis on the hypercube, we lose one degree of freedom,
%     and as a stand-in for distance, we use the negative definite kernel metric 
%     previously explored in~\cite{trubey:pg} as a means of establishing a proper 
%     scoring rule on $\mathbb{S}_{\infty}^{d-1}$.  Thus, the score becomes
%     \begin{equation}
%         \label{eq:ad_knn_h}
%         S_i^{hKNN} = \frac{r_i^{2}N}{k}
%         \frac{\pi^{\frac{d-1}{2}}D_{k}(\bm{v}_i)^{d-1}}{
%           \Gamma\left(\frac{d-1}{2} - 1\right)}
%   \end{equation}
%     This is of course an approximation to the true posterior predictive density 
%     at observation $i$.  In addition to the noise induced by using random 
%     samples from the posterior predictive, there is some bias induced in the 
%     calculated volume
%     if $D_k(\bm{v}_i)^{d-1}$ is greater than the distance from $\bm{v}_i$ to 
%   unity ($\bm{1}_d$), or from $\bm{v}_i$ to some axis.  In practice,
%   this induced bias seems to have relatively small effect. 
%   \makenote{worth looking into what proportion of observations experience this 
%   bias, and what performance outcome is}.
%   In our experience, using a large posterior predictive sample, the resulting 
%   ordering of scores in $k$NN based methods was robust to the choice of $k$, for 
%   values between 2 and 10.

\subsection{Kernel Density Estimation}
\makenote{needs citation for KDE}

If we assume a Gaussian kernel, then the KDE density for a generic $\bm{x}$ can 
    be approximated as 
    \begin{equation}
        \label{eq:ad_kde}
        f_{\bm{x}}^{(\text{KDE})}(\bm{x}_i) \propto 
        \frac{1}{N}\sum_{j = 1}^{N}\exp
        \left\lbrace-\frac{1}{2}\left(
        \frac{D(\bm{x}_i,\bm{X}_j)}{h}\right)^2\right\rbrace
    \end{equation}
    where $\bm{X}_j$ is the $j$th 
    replicate from $\bm{X}\mid\bm{x}$.  The \emph{bandwidth} parameter $h$ 
    serves to smooth the calculated density, with larger values creating a 
    larger smoothing effect.  Bandwidth selection is a vast field of study, and 
    its nuances lie outside the scope of this work.  As in general KDE can be 
    expressed using a multivariate normal density, the bandwidth can be 
    expressed as a covariance matrix.  In such a case, the log-score is 
    effectively the negative squared Mahalanobis distance.  For our purpose, as 
    distance on $\mathbb{S}_{\infty}^{d-1}$ is less straightforward, we must 
    restrict ourselves to the univariate representation outlined above.

To develop a bandwidth parameter, we employ a univariate variation on 
    Silverman's rule of thumb\makenote{find citation}
    \begin{equation}
        \label{eqn:silverman}
        h = \left(\frac{4}{d+2}\right)^{
            \frac{1}{d+4}}n^{-\frac{1}{d+4}}\hat{\sigma}.
    \end{equation}
    This then requires the estimation of $\hat{\sigma}$, which must we calculate 
    from pairwise distances. Recall that for a random variable $X$, 
    \[ 
        \text{E}\left[\left\lVert X_j - X_k\right\rVert_2\right] 
            = 2\text{Var}(X). 
    \]
    We develop our estimator for $\hat{\sigma}$ in a similar manner.  Let
    \[
        \hat{\sigma} = \sqrt{\frac{1}{2N(N-1)}\sum_{j\neq k}\lVert 
            \bm{X}_j - \bm{X}_k\rVert_2},
    \]
    where $\bm{X}_j$, $\bm{X}_k$ are both replicates of $\bm{X}\mid\bm{x}$. Then 
    $\hat{\sigma}$ is employed in Equation~\ref{eqn:silverman} to produce $h$.
 
\begin{equation}
    \label{eqn:ad_kde_h}
    S_i^{\text{hKDE}} = r_i^{2}\text{E}\left[\exp\left\lbrace -
    \left(\frac{d(\bm{v}_i,\bm{V})}{h}\right)^2\right\rbrace\mid\bm{v}\right]
\end{equation}

% EOF
