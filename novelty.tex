\section{Novelty Detection Methods\label{sec:novelty}}
As previously stated, a novelty detection algorithm produces an anomaly score 
    which provides a ranked ordering of observations in their likelihood of being
    anomalous, with higher scores indicating more likely anomalous. Building on 
    the notion that anomalies occur in areas of low density, a general Bayesian
    anomaly score for observation $x_i$, can be defined as
    \[
    \score_i = \left[\int_{\Theta}f(x_i\mid\theta)dG(\theta\mid\mathcal{D})\right]^{-1}
    \]
    where $\mathcal{D}$ is the observed data and $\theta$ the distributional
    parameters.  That is, the reciprocal of the posterior predictive density 
    at observation $x_i$.

Given the independence between the angular and radial components of an
    extreme observation, we can consider sub-scores for the radial 
    and angular components independently.  That is,
    \[
        \score_i = \score_{i,r}\times \score_{i,\bm{v}} = 
            f_r(r_i)^{-1}\times
            \left[\int_{\Omega} f_v(\bm{v}_i\mid\bm{\alpha})
                \,\text{d}G(\bm{\alpha}\mid\mathcal{D})\right]^{-1}
    \]
    By construction $r_i$ follows a standard Pareto distribution, so its
    density is $f_r(r_i) = r_i^{-2}$.  As previously discussed in
    Section~\ref{sec:evt}, the kernel $\mathcal{PG}_\infty$, needed for 
    density estimation on the surface of ${\mathbb S}_{\infty}^{d-1}$ is 
    not available in analytic form, thus, we resort to transforming the
    data to $\mathbb{S}_p^{d-1}$ for a large but finite $p$.
    This makes estimation of distributional parameters possible, 
    but in the context of anomaly detection, a score based on
    $\mathcal{PG}_p$, for any $p$, is problematic.  In fact, the 
    transformation from ${\mathbb R}_+^p$ to $\mathbb{S}_p^{d-1}$ is not
    unique, as we can take any of the components of the original vector
    as a reference.  This implies that
    under uniform $\bm{\alpha}$, the density can be changed by
    permuting the order of components.  This is not appropriate for anomaly
    detection, because a relative ordering of density between 
    observations is specifically what we're trying to calculate. In addition
    we have observed instabilities in the evaluation of (\ref{eqn:projgamma}) 
    for small arguments, when the shape parameter is small. On the other hand,
    we notice that $T_\infty$ is unique, as the reference is the largest value
    of the array. Thus, we fit the mixture model in $\mathbb{S}_p^{d-1}$, generate
    posterior predictive samples, and transform those samples to 
    $\mathbb{S}_\infty^{d-1}$.

To avoid the problems of angular density evaluation in $\mathbb{S}_\infty^{d-1}$ we use a 
    non-parametric angular density estimator based on a sample from the 
    posterior predictive distribution of the model developed in 
    Section~\ref{sec:evt}. Here, we consider two well-established
    methods: $k$--nearest neighbors, or \emph{$k$NN} \citep{mack1979}, and 
    kernel density estimation, or \emph{KDE} \citep{parzen1962}.  For both of 
    these methods we make use of pairwise distances between observations 
    from the dataset, and replicates from a posterior predictive sample.  

As described in \cite{trubey:pg}, the  geodesic distance on 
    $\mathbb{S}_{\infty}^{d-1}$ 
    is expensive to evaluate as the computational burden grows 
    combinatorically with the number of dimensions.  As an alternative they propose an 
    estimate of distance that is computationally cheap to evaluate, bearing 
    a cost equivalent to that of a Euclidean norm.
    Let
    ${\mathbb C}_{\ell}^{d-1} = \lbrace \bm{x} : 
        \bm{x} \in {\mathbb S}_{\infty}^{d-1}, x_{\ell} = 1\rbrace$
    comprise the $\ell$th \emph{face} of $\mathbb{S}_{\infty}^{d-1}$.  For a 
    pair of points on the same face, the Euclidean distance corresponds to the
    geodesic, or length of the shortest possible path between those two points.  
    For a pair of points 
    $\bm{a} \in \mathbb{C}_{\ell}^{d-1},\;\bm{b}\in\mathbb{C}_{\jmath}^{d-1}$,
    we can rotate $\mathbb{C}_{\jmath}^{d-1}$ into the same hyperplane as 
    $\mathbb{C}_{\ell}^{d-1}$.  Transform $\bm{b}$ such that %\\
    \begin{equation}
        \label{eqn:rotation}
        \bm{b}^{\prime} = P_{\jmath\ell}(\bm{b}) = 
        \begin{cases}
            b_{i} &\text{for }i\neq \jmath,\ell\\
            1 &\text{for }i = \ell\\
            2 - b_{\ell} &\text{for }i = \jmath
        \end{cases}\;\hspace{2cm}\;
        g(\bm{a},\bm{b}) = \lVert \bm{a} - \bm{b}^{\prime}\rVert_2
    \end{equation}
    After transformation, the Euclidean norm between $\bm{a}$ and 
    $\bm{b}^{\prime}$ corresponds to a negative definite kernel that
    provides an upper bound on geodesic distance on 
    $\mathbb{S}_{\infty}^{d-1}$ between $\bm{a}$ and $\bm{b}$.

\subsection{Nearest Neighbor Density estimation}
We use the kernel $g$ defined in Equation \ref{eqn:rotation} to obtain 
    a local posterior predictive density based on a $k$NN estimator 
    on $\mathbb{S}_{\infty}^{d-1}$. To this end we consider a locally 
    uniform density within a $d-1$--dimensional ball $\mathbb{B}$, centered on 
    observation $\bm{v}_i$.  The radius $D_{k}(\bm{v}_i)$ is calculated as 
    $g\left(\bm{v}_i, \bm{v}_{N_k(i)}^*\right)$, where $\bm{v}_{N_k(i)}^*$ is the $k$th nearest 
    neighbor of $\bm{v}_i$ in a sample from the 
    posterior predictive distribution. The volume of the ball is calculated as
    \begin{equation}
        \label{eqn:vol_sphere}
        \text{Vol}(\mathbb{B}_k^{d-1}) =
        \frac{\pi^{\frac{d-1}{2}}D_{k}(\bm{v}_i)^{d-1}}{
            \Gamma\left(\frac{d-1}{2} + 1\right)}.
    \end{equation}
    The density is thus estimated as 
    $f_{\bm{v}}^{(k\text{NN})}(\bm{v}_i\mid \bm{V}) \approx 
        \frac{k}{N}\left(\text{Vol}(\mathbb{B}_{k}^{d-1})\right)^{-1}$
    where $N$ is the total number of replicates of from the posterior predictive
    distribution.  Taking the reciprocal of the estimated angular density, 
    the angular score under the $k$NN estimator is then
    \begin{equation}
        \label{eqn:ad_knn_h}
        \score_{i,\bm{v}}^{\text{$k$nn}} = 
            \frac{N\pi^{\frac{d-1}{2}}D_{k}(\bm{v}_i)^{d-1}}{
            k\Gamma\left(\frac{d-1}{2} - 1\right)}
    \end{equation}
    In our experience, using a large posterior predictive sample,
    the resulting ordering of scores was relatively robust to a choice of $k$ 
    between 2 and 10.  We used $k = 5$ in our performance analysis.

\subsection{Kernel Density Estimation}
Kernel density estimation is an approach that makes use of 
    kernel smoothing to produce a semi-parametric estimate of the density
    function for a dataset.  For a scalar bandwidth parameter $h$,
    \[
        f_n(x) \;=\; 
            \int_{\Omega}\frac{1}{h}
                \mathcal{Q}\left(\frac{ \bm{x} - \bm{t}}{h}\right)dF_n(\bm{t}) 
            \;\approx\;
            \frac{1}{Kh}\sum_{k = 1}^K
                \mathcal{Q}\left(\frac{ \bm{x} - \bm{x}_k^{*}}{h}\right)
    \]
    where $\bm{x}_k^{*}$ are random replicates from $F$.  The choice of kernel function 
    $\mathcal{Q}$, and selection of the bandwidth parameter $h$ are both topics that have
    been extensively researched. In practice the Gaussian kernel seems to be 
    well regarded for its simplicity, flexibility, and interpretability.  The
    bandwidth parameter in this case corresponds to the standard deviation 
    of the kernel function.  The multivariate Gaussian kernel is more flexible,
    accepting a matrix as the bandwidth parameter.  A larger bandwidth serves 
    to smooth the resulting density estimate, where a lower bandwidth is more 
    responsive to individual observations of data.  Optimization of $h$ is 
    application and data specific, but there do exist various 
    \emph{rules of thumb} based on summary statistics of the data. For our 
    analysis, we are making use of a distance analogue on 
    $\mathbb{S}_{\infty}^{d-1}$ described in Equation~(\ref{eqn:rotation}), 
    which precludes the ability to describe bandwidth using a matrix.  We 
    therefore consider the univariate case of $f$ in kernel space, where 
    $\lVert x - x^*\rVert$ has been replaced with $g(\bm{v}, \bm{v}^*)$.

For selection of the bandwidth parameter $h$, we employ Silverman's rule of
    thumb \citep{silverman2018}, estimating 
    $\hat{h} = \left(\frac{4}{d+2}\right)^{\frac{1}{d+4}}
        n^{-\frac{1}{d+4}}\hat{\sigma}$.
    This then requires the estimation of $\hat{\sigma}$, which in this case we
    calculate from pairwise distances.  Recall that for a random variable $X$,
    $\text{E}\left[\left\lVert X_j - X_k\right\rVert_2\right] 
        = 2\text{Var}(X)$.
    In that case, $\hat{\sigma} = 
        \sqrt{\frac{1}{2N(N-1)}\sum_{j\neq k}g(\bm{v}_j^*,\bm{v}_k^*)}$, where
    $\bm{v}_j^*,\bm{v}_k^*$ are replicates from the posterior predictive distribution.
    Then $\hat{\sigma}$ is used in the aforementioned rule of thumb for $h$.
    Finally, the angular score under KDE is then calculated as
    \begin{equation}
    \label{eqn:ad_kde_h}
    \score_{i,\bm{v}}^{\text{kde}} = \text{E}_{\bm{v}^*}\left[\exp\left\lbrace -
    \left(\frac{g(\bm{v}_i,\bm{v}^*)}{\hat{h}}\right)^2\right\rbrace\right]^{-1}
    \approx
    \left[\frac{1}{K}\sum_{k = 1}^{K}
    \exp\left\lbrace-\left(\frac{g(\bm{v}_i,\bm{v}_k^*}{\hat{h}}\right)^2\right\rbrace\right]^{-1}
    \end{equation}
    where $\bm{v}_k^*$ are again replicates from the posterior predictive distribution.
    We investigated other methods of calculating bandwidth, as well as searched
    the neighborhood around our bandwidth estimate for example datasets.
    The estimator following Silverman's rule of thumb as described consistently 
    produced the most performant rank ordering of angular anomaly scores on 
    tested datasets.

\begin{algorithm}[htb]
    \caption{Workflow for anomaly detection on $\mathbb{S}_{\infty}^{d-1}$.}\label{alg:adreal}
    \begin{algorithmic}[1]
        \State Take $r_i$, $\bm{y}_i$ according to Algorithm~(\ref{alg:workflow})
        \State Fit $\bm{y}\sim\mathcal{PYPG}$ from Equation~(\ref{eqn:modelsphere})
        \State From $\bm{\alpha}\mid\bm{y}$, sample
            $\bm{\varrho}_k^{*}\mid\bm{\alpha}\sim\prod_{\ell}\mathcal{G}(\alpha_{\ell})$ for $k = 1,\ldots,K$
        \State Take $\bm{v}_k^{*} = T_{\infty}(\bm{\varrho}_k^{*})$
        \State Take $\score_{i,v}$ as per Equations~(\ref{eqn:ad_knn_h},\ref{eqn:ad_kde_h})
    \end{algorithmic}
\end{algorithm}

% EOF
