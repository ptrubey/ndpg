\section{Novelty Detection Methods\label{sec:novelty}}
As previously stated, a novelty detection algorithm produces an anomaly score 
    which provides a ranked ordering of which observations are likely to be 
    anomalous, with higher scores indicating more likely anomalous. Building on 
    the notion that anomalies occur in areas of low density, a general score 
    might establish for observation $x_i$, 
    \[S_i = \text{E}\left[f_{x}(x_i\mid\mathcal{D})^{-1}\right]\]
    where $\mathcal{D}$ is the observed data.  That is, the reciprocal of the 
    posterior predictive density at observation $x_i$.

With independence established between the angular and radial components of an
    extreme observation, we can consider sub-scores established under radial 
    and angular components independently.  That is,
    \[
        S_i = S_i^{(r)}\times S_i^{(\bm{v})} = 
            \text{E}\left[f_r(r_i\mid\mathcal{D})^{-1}\right]\times
            \text{E}\left[f_v(\bm{v}_i\mid\mathcal{D})^{-1}\right]
    \]
    As we have established, $r_i$ follows a standard Pareto distribution, so its
    density $f_r(r_i) = r_i^{-2}$.  Estimating the angular density, 
    $f_v(\bm{v}_i)$, is more complicated.  As we have previously established in
    Section~\ref{sec:evt}, density estimation on the surface of 
    ${\mathbb S}_{\infty}^{d-1}$ is not an easy problem. To establish a method
    of inferring such a distribution, we instead projected the data from 
    $\mathbb{S}_{\infty}^{d-1}$ onto $\mathbb{S}_p^{d-1}$ for a large but finite
    $p$.  This makes estimation of distributional parameters possible, but a
    calculated density for a given observation under a finite $p > 1$ depends on
    which dimension $\ell \in \lbrace 1,\ldots,d\rbrace$ was used as the summary
    variable.  Calculated density, and thus rank ordering of anomaly scores,
    could change depending on which dimension was used.  This is not ideal.
    Further, for an observation near 0 evaluated on a gamma-related distribution 
    with a shape parameter $\alpha$ near 0 creates an evaluated density that 
    tends towards infinity.  This instability would play havoc on directly 
    calculated density-based scores.

We sidestep these problems of angular density evaluation by establishing a 
    non-parametric angular density estimator, using as input a sample from the 
    posterior predictive distribution of the model established in 
    Section~\ref{sec:evt}. By fitting an angular model to the data, we can 
    generate an arbitrarily large sample with which to establish the 
    non-parametric density estimator.  Here, we consider two well-established
    methods: $k$-nearest neighbors, or \emph{$k$NN}, and 
    kernel density estimation, or \emph{KDE}.  Both of these methods make
    use of pairwise distances between the observed values to evaluate, and
    observations from an input dataset.  We use a sample from the posterior
    predictive distribution as that input dataset.  

As described in \cite{trubey:pg}, \emph{distance} on $\mathbb{S}_{\infty}^{d-1}$ 
    is computationally expensive to evaluate.  Instead, they propose a kernel
    metric that is computationally cheap to evaluate, bearing a cost
    equivalent to Euclidean norm.  For observations on different faces of 
    $\mathbb{S}_{\infty}^{d-1}$, a performant analogue to distance can be
    evaluated by rotating the face of the second observation along the axis of
    the first observation, and computing the Euclidean norm between the first
    observation and the transformed second observation.  Indeed, this metric can
    be considered an upper bound on distance on $\mathbb{S}_{\infty}^{d-1}$
    between the two observations.

\add{\bf This whole section needs to rewritten. First we have to connect to 
    the previous section. As a result of what the methods presented in the 
    previous section we have a sample from the angular density that 
    characterizes the extreme behavior of our observations. We have samples, 
    but we have no way to evaluate it. At this point you provide information 
    about estimated of the density, and then you have to mention that a 
    measure of distance on ${\mathbb S}_{\infty}^{d-1}$, and you refer to the 
    previous paper.}

% \makenote{A reviewer might ask about $p=1$, and using the direct density 
%   calculated thereof.  We should add that fitted gamma densities are inherently 
%   unstable and the spiking behavior near axes results in \emph{extremely} 
%   non-normal behavior.  True density metrics based on projected gamma (at any $p$) 
%   are not going to behave properly and are undesireable.}

\subsection{$k$-Nearest Neighbors Density estimation}
We establish a local posterior predictive density under the $k$-Nearest Neighbor 
    estimator \citep{mack1979} by assuming a locally uniform density within a 
    $d$--dimensional ball $\mathbb{B}$, centered on observation $i$, with 
    radius $D_{k}^d(x_i)$ calculated as the distance to the $k$th nearest 
    neighbor to observation $i$ in a sample from the posterior predictive 
    distribution. The volume of the ball is calculated as
    \begin{equation}
        \label{eq:vol_sphere}
        \text{Vol}(\mathbb{B}_d^k) =
        \frac{\pi^{\frac{d}{2}}D_{k}(\bm{x}_i)^d}{
            \Gamma\left(\frac{d}{2} + 1\right)}
    \end{equation}
    where $D_k(\bm{x}_i)$ is the distance from $\bm{x}_i$ to the $k$th nearest 
    replicate of $\bm{X}\mid\bm{x}$.  A density is established as 
    \begin{equation}
        \label{eq:ad_knn}
        f_{x}^{(k\text{NN})}(x_i\mid X) \approx 
            \frac{k}{N}\left(\text{Vol}(\mathbb{B}_d^k\right)^{-1}
    \end{equation}
    where $N$ is the total number of replicates of $\bm{X}\mid\bm{x}$.  As 
    mentioned, we desire a score interpreted as higher is more likely anomalous, 
    so we take the score as the reciprocal of the density.

When implementing this analysis on the hypercube, we lose one degree of freedom,
    and as a stand-in for distance, we use the negative definite kernel metric 
    previously explored in~\cite{trubey:pg} as a means of establishing a proper 
    scoring rule on $\mathbb{S}_{\infty}^{d-1}$.  Thus, the score becomes
    \begin{equation}
        \label{eq:ad_knn_h}
        S_i^{hKNN} = \frac{r_i^{2}N}{k}
        \frac{\pi^{\frac{d-1}{2}}D_{k}(\bm{v}_i)^{d-1}}{
          \Gamma\left(\frac{d-1}{2} - 1\right)}
  \end{equation}
    This is of course an approximation to the true posterior predictive density 
    at observation $i$.  In addition to the noise induced by using random 
    samples from the posterior predictive, there is some bias induced in the 
    calculated volume
    if $D_k(\bm{v}_i)^{d-1}$ is greater than the distance from $\bm{v}_i$ to 
  unity ($\bm{1}_d$), or from $\bm{v}_i$ to some axis.  In practice,
  this induced bias seems to have relatively small effect. 
  \makenote{worth looking into what proportion of observations experience this 
  bias, and what performance outcome is}.
  In our experience, using a large posterior predictive sample, the resulting 
  ordering of scores in $k$NN based methods was robust to the choice of $k$, for 
  values between 2 and 10.

\subsection{Kernel Density Estimation}
\makenote{needs citation for KDE}

If we assume a Gaussian kernel, then the KDE density for a generic $\bm{x}$ can 
    be approximated as 
    \begin{equation}
        \label{eq:ad_kde}
        f_{\bm{x}}^{(\text{KDE})}(\bm{x}_i) \propto 
        \frac{1}{N}\sum_{j = 1}^{N}\exp
        \left\lbrace-\frac{1}{2}\left(
        \frac{D(\bm{x}_i,\bm{X}_j)}{h}\right)^2\right\rbrace
    \end{equation}
    where $\bm{X}_j$ is the $j$th 
    replicate from $\bm{X}\mid\bm{x}$.  The \emph{bandwidth} parameter $h$ 
    serves to smooth the calculated density, with larger values creating a 
    larger smoothing effect.  Bandwidth selection is a vast field of study, and 
    its nuances lie outside the scope of this work.  As in general KDE can be 
    expressed using a multivariate normal density, the bandwidth can be 
    expressed as a covariance matrix.  In such a case, the log-score is 
    effectively the negative squared Mahalanobis distance.  For our purpose, as 
    distance on $\mathbb{S}_{\infty}^{d-1}$ is less straightforward, we must 
    restrict ourselves to the univariate representation outlined above.

To develop a bandwidth parameter, we employ a univariate variation on 
    Silverman's rule of thumb\makenote{find citation}
    \begin{equation}
        \label{eqn:silverman}
        h = \left(\frac{4}{d+2}\right)^{
            \frac{1}{d+4}}n^{-\frac{1}{d+4}}\hat{\sigma}.
    \end{equation}
    This then requires the estimation of $\hat{\sigma}$, which must we calculate 
    from pairwise distances. Recall that for a random variable $X$, 
    \[ 
        \text{E}\left[\left\lVert X_j - X_k\right\rVert_2\right] 
            = 2\text{Var}(X). 
    \]
    We develop our estimator for $\hat{\sigma}$ in a similar manner.  Let
    \[
        \hat{\sigma} = \sqrt{\frac{1}{2N(N-1)}\sum_{j\neq k}\lVert 
            \bm{X}_j - \bm{X}_k\rVert_2},
    \]
    where $\bm{X}_j$, $\bm{X}_k$ are both replicates of $\bm{X}\mid\bm{x}$. Then 
    $\hat{\sigma}$ is employed in Equation~\ref{eqn:silverman} to produce $h$.
 
\begin{equation}
    \label{eqn:ad_kde_h}
    S_i^{\text{hKDE}} = r_i^{2}\text{E}\left[\exp\left\lbrace -
    \left(\frac{d(\bm{v}_i,\bm{V})}{h}\right)^2\right\rbrace\mid\bm{v}\right]
\end{equation}

% EOF
