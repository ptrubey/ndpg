%\subsection{Review of Novelty Detection Regimes}

The complete field of anomaly detection is vast.  However, most methods can be 
    roughly grouped into three core ideas: statistical model approaches, 
    non-statistical model approaches, and clustering methods.  Common to all
    approaches is the assumption that anomalous data will tend to stand
    apart from normal data.

%{\bf Statistical Models} 
Statistical models for anomaly detection attempt to model the distribution 
    of data, with the goal of estimating the data density around an observation.
    In specific applications, one might
    make assumptions about the parametric form of the generating distribution
    of the data, but for general application, a non-parametric density estimator
    is frequently used.  This might include algorithms such as
    $k$-Nearest Neighbors  \emph{$k$-NN}~\citep{kramer2013}; 
    kernel density estimation approaches such as the Parzen-Rosenblatt 
    windowing method~\citep{rosenblatt1956,parzen1962}; 
    or even semi-parametric density estimation methods, such as Gaussian mixture 
    models~\citep{mcnicholas2010}.  Local Outlier Factor~\cite{breunig2000} is 
    an example of an anomaly score using a non-parametric density estimator.

%{\bf Clustering Methods} 
Clustering methods group data into clusters of similar observations.
    The grouping methods tend to rely on distance metrics and generally make no 
    assumptions regarding the underlying distribution of the data.  We can 
    further sub-divide this sub-field into types of clustering methods: 
    linkage-based, centroid-based, and density-based. These methods as applied 
    to the field of anomaly detection assume that anomalous observations tend 
    to stand apart from non-anomalous data.  
    
Linkage-based clustering methods group data based on pairwise distance 
    point-to-point, or between elements of clusters.  \cite{ackerman2010} offers
    a review of the topic.  An illustrative example is single linkage, where 
    the distance between two clusters is defined as the minimum distance 
    between a point in each set.   Similarly, complete linkage defines the 
    metric to be the maximum pairwise between a point in each set.  The goal of 
    the linkage-based clustering algorithm is to maximize the total distance 
    between clusters under whatever metric of distance is used, along with 
    minimizing distance within clusters.  An observation's anomaly score might 
    be a function of distance to its nearest neighbor within its assigned 
    cluster.

Centroid based clustering methods instead generate cluster centroids 
    according to some metric.  The algorithm used to find the cluster centroids 
    depends on a chosen metric.  The very popular $k$-Means~\citep{hartigan1979} 
    is an example of this approach. Under $k$-Means, cluster assignment is 
    determined by minimizing within-cluster distance among $k$ clusters, which 
    simultaneously maximizes between-cluster distance. For each observation, 
    and anomaly score may be obtained as a function of its distance to the 
    nearest cluster centroid.  
    
Density based clustering methods use pairwise distances between observations to 
    establish a measure of local density, then establish local modes as 
    clusters.  \emph{DBSCAN}~\citep{ester1996} follows this approach, forming 
    neighborhoods of observations and assigning labels based on the 
    neighborhood. 
    
% {\bf Non-Statistical Models} 
Non-statistical---or algorithmic---models beyond clustering are generally adaptations 
    of general classification methods, applied to unsupervised learning.  
    The Isolation Forest~\citep{liu2000}, adapted from random 
    forests~\citep{breiman2001}, uses decision trees to isolate observations.
    Those observations that are more easily isolable are regarded as more 
    anomalous.  One-class Support Vector Machines~\citep{chang2011} is a variant
    of the support vector machine classification system, optimized for 
    anomaly detection.  One-class SVM uses support vectors to describe a 
    decision boundary in kernel space around \emph{normal} behavior. A higher 
    distance to that decision boundary on the anomalous side is regarded as 
    more anomalous.

The applicability of extreme value theory to anomaly detection is predicated on 
    the assumption that extreme observations are more likely to be anomalous.  
    A discussion on this point is provided by \cite{goix2017}, stating that 
    extreme observations exist at the border between anomalous and non-anomalous 
    regions.  Indeed, for most datasets in our testing, the probability an 
    individual observation is anomalous is higher for data in the tails of the 
    distribution. This relative abundance of anomalies among extremes might 
    cause a naive classifier that does not take into account the dependence 
    structure of extremes to classify all extremes as anomalous.  If we follow 
    the assumption that anomalies stand apart, then extreme observations that 
    cluster into a homogenous group should not be considered anomalous.  For 
    this reason, we desire a classifier that considers the dependence structure 
    of the extremes as well.
    
The intersection of extreme value theory and anomaly detection is a current 
    topic of research.  Some methods employ univariate EVT on estimated 
    densities calculated via other means, such as \cite{clifton2011} using a 
    Gaussian Mixture model, and \cite{gu2021} using a Gaussian process.  Both 
    then employ EVT on the estimated densities to establish a decision 
    threshold theoretically, avoiding the process of determining said
    threshold heuristically.
\st{The application of multivariate extreme value theory to anomaly detection is 
    based on the  assumption that extreme observations are more likely to be 
    anomalous, as argued in }\st{In fact, extreme observations 
    exist at the border between anomalous and non-anomalous regions, and it is 
    often the case that the probability an individual observation being 
    anomalous is higher for data in the tails of the distribution.
%    \makenote{duplicated sentence from earlier in introduction.  
%        which one stays?} 
    This relative abundance of anomalies among extremes might cause a naive 
    classifier, that does not take into account the dependence structure of 
    extremes, to classify all extremes as anomalous. Yet, extreme observations 
    that cluster into a homogenous group should not be considered anomalous.}
    \cite{goix2017} offers an example of the use of the dependence structure 
    of extremes. Their method is based on transforming the data to a standard 
    Pareto suing the transformation  $T(x) = 1/(1 - \hat{F}(x))\;\in\;[1,\infty)$, 
    where $\hat{F}$ corresponds  to the empirical distribution function. Then the 
    the space $[1,\infty)^d$ is partitioned into $\alpha$-cones, defined as 
    subsets where in each dimension the observations are in excess of a $\alpha$.
    $\alpha$-cones with few observations correspond to lower-density regions, so 
    observations  falling into these cones are considered more likely to be anomalous.

A central result of multivariate EVT is that, conditional on an observation 
    being extreme, its radial component---or magnitude---is independent of its 
    angular component.  In this paper, following \cite{trubey:pg}
    we fit a Bayesian non-parametric mixture 
    of projected gammas to the angular component, and use samples from its 
    posterior predictive distribution to compute an estimate of the density 
    of the angular component.   Direct estimation of density via a fitted model
    is difficult, owing to the bounded nature of the angular distribution.  
    Instead, we employ non-parametric density estimators including $k$-nearest 
    neighbors and kernel density estimation to produce estimates of angular 
    density.  Further, to expand the applicability of this algorithm, we 
    produce an extension of the BNP projected gamma model to include categorical 
    data.   Standing alone, this component represents a highly flexible density 
    model for categorical data, and it efficiently pairs with the projected 
    gamma model for angular data.  We develop several anomaly scoring metrics 
    applicable to the angular data, categorical data, and \emph{mixed} data 
    regimes.  The major contributions of this paper are thus three-fold: We 
    develop an anomaly detection algorithm for extreme data that accounts for 
    the dependence structure between extremes, approaching density estimation 
    in a continuous space  rather than discrete binning in a partition of the 
    space. We obtain a flexible model for multivariate categorical data that 
    efficiently captures the dependence structure between categories in multiple 
    variables, as well as anomaly scores in this setting. Finally, we 
    provide a model that links the scores developed in these two cases, tackling
    multivariate observations with components of different types.

The paper proceeds as follows: Section~\ref{sec:evt} provides a 
    brief review of multivariate EVT, explaining the separation of the radial 
    and angular components of the extreme data, as well as an introduction of 
    the angular data model.  Section~\ref{sec:novelty} introduces our anomaly 
    scores for angular data, describing the density estimation methods employed, 
    as well as how radial information is incorporated.  
    Section~\ref{sec:categorical} introduces our flexible categorical data 
    model, along with anomaly scores based on it. 
    Section~\ref{sec:mixedscores} provides a link between the two regimes; 
    anomaly scores that include information from both categorical and angular 
    data.  
    Section~\ref{subsec:rank} employs the same rank transformation used in 
    \cite{goix2017} to apply the angular data model to data not already assumed 
    to be in excess of a threshold, widening the applicability of our metrics.
    Section~\ref{sec:results} provides the resulting performance of our anomaly 
    scores as applied to seven reference anomaly detection datasets, as well as 
    comparing to three canonical anomaly scoring methods.  
    Finally, Section~\ref{sec:conclusion} provides concluding remarks and discussion.

 % EOF