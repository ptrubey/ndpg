\subsection{Review of Novelty Detection Regimes}

The complete field of anomaly detection is wide.  However, most methods can be 
    roughly grouped into three core ideas: statistical model approaches, 
    non-statistical model approaches, and clustering methods. Core to the 
    development of a method lies a basic question, \emph{what is an anomaly?}  
    The answer to this question depends on application and what we hope to
    learn.  For instance, in a regression setting, observations that produce 
    large outliers, or exhert outsized influence on parameter estimates, might 
    be considered anomalous.  In a classification setting, common to most 
    methods of anomaly detection is the belief that anomalous data is in some 
    manner \emph{different} than \emph{normal} data. This belief is extrapolated
    into the assumption that data belonging to regions of relative data sparsity
    are more likely to be anomalous.  Methods in this field work in some fashion
     to identify these regions of relative data sparsity.

\paragraph{Statistical Models} for anomaly detection attempt to model the 
    distribution of data, with the goal that the model can be to estimate the 
    data density around an observation. The assumption made is that observations
    in regions of low estimated density are more likely to be anomalous.  This 
    can include non-parametric density estimation routines such as the 
    $k$-Nearest Neighbors, or \emph{$k$-NN}~\citep{kramer2013}; kernel density 
    estimation such as the Parzen-Rosenblatt windowing 
    method~\citep{parzen1962,rosenblatt1956}.  This can also include parametric 
    density estimation routines, such as Gaussian mixture 
    models~\citep{mcnicholas2010}.

\paragraph{Clustering Methods} tend to sort data into groups \emph{near} 
    each-other.  These rely on \emph{distance} metrics and tend to make no 
    assumptions regarding the underlying distribution of the data.  We can 
    further sub-divide this sub-field into density-based, centroid-based, and 
    linkage-based clustering methods.  For all of these methods, we can regard 
    observations with less association to their identified cluster as being more
    anomalous.

Linkage-based clustering methods group data based on pairwise distance 
    point-to-point, or between elements of clusters.  \cite{ackerman2010} offers
    a review of the topic.  An illustrative example is single linkage, where 
    the distance between two clusters is defined as the minimum distance 
    between a point in each set.   Similarly, complete linkage defines the 
    metric to be the maximum pairwise between a point in each set.  The goal of 
    the linkage-based clustering algorithm is to maximize the total distance 
    between clusters under whatever metric of distance is used, along with 
    minimizing distance within clusters.  An observation's anomaly score might 
    be a function of distance to its nearest neighbor within its assigned 
    cluster, or distance to its assigned cluster's centroid.

Centroid based clustering methods instead generate $k$ cluster centroids 
    according to some metric, then an observation's \emph{anomaly} score is a 
    function of the distance from that observation to the nearest cluster 
    centroid.  The algorithm used to find the cluster centroids is also 
    variable, changing what metric is optimized in finding the location of the
    centroids.  The very popular \emph{$k$-Means} \citep{hartigan1979} is an 
    example of this type of method.  Under $k$-Means, cluster assignment is 
    decided by minimizing within-cluster distance, which for a given $k$ 
    simultaneously maximizes between-cluster distance.

Density based clustering methods will use pairwise distances to establish some 
    measure of local density, then establish local modes as clusters.
    \emph{DBSCAN}~\citep{ester1996} follows this, by forming neighborhoods of 
    observations and establishing labels based on the neighborhood.

\paragraph{Non-Statistical methods} beyond clustering are generally adaptations 
    of general classification methods here applied to unsupervised learning.  
    The Isolation Forest~\citep{liu2000}, adapted from random 
    forests~\citep{breiman2001}, uses decision trees to isolate observations.
    Those observations that are more easily isolable are regarded as more 
    anomalous.  One-class SVM~\citep{chang2011} is a variant of the support 
    vector machine classification system optimized towards anomaly detection, 
    where in a similar manner to the support vector machine using support 
    vectors to describe a decision boundary in kernel space separating classes 
    of data, the one-class SVM uses support vectors to describe a decision 
    boundary in said kernel space around \emph{normal} behavior. A higher 
    distance to that decision boundary on the anomalous side is regarded as 
    more anomalous.

 % EOF