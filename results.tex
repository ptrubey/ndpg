\section{Results\label{sec:results}}
As mentioned in~\ref{sec:novelty}, our goal is 
    to produce novelty scores to rank observations according to how likely 
    they are of being anomalous.  This creates another problem: threshold 
    selection---anomaly scores beyond what level are determined anomalous?  We 
    mentioned \cite{clifton2011} and \cite{gu2021} as examples of computing 
    thresholds theoretically, but in general, thresholds are determined 
    heuristically, using performance criteria.  In some applications, 
    heuristic determination can be extremely costly.

One such criteria is the receiver operating characteristics, or \emph{ROC}, 
    curve.  For  a given score threshold, one can compute the true positive 
    rate, or TPR, as the number of anomalous observations with scores above the 
    threshold, divided by the total number of anomalous observations.  The 
    false positive rate, or FPR, is similarly the number of non-anomalous 
    observations above the threshold, divided by the total number of 
    non-anomalous observations.  The ROC curve is formed as the TPR is plotted 
    on the vertical axis against the FPR on the horizontal axis for a range of 
    possible thresholds.  The curve is non-decreasing, starting at the origin 
    $(0,0)$, and ending at unity $(1,1)$.  Threshold selection might include 
    specifying an acceptable FPR, and determining the threshold that produces 
    that FPR.

In assessing model performance, we sideline the issue of threshold selection by 
    observing the whole ROC curve.  Specifically, we look for the area under 
    the ROC curve, \emph{(AuROC)}.  The better a classifier is, the closer its 
    ROC curve will approach the upper left corner, and the closer its AuROC will 
    approach 1.

In developing our model, we employ the blocked Gibbs sampler for stick-breaking
    priors detailed in~\cite{ishwaran2001}.  We set a discount factor of $0.1$, 
    and a concentration parameter of $1.0$.  In our testing, in the neighborhood
    around these values we found the resultant number of extant clusters to be 
    relatively stable.  We use $(\mu_0 = \bm{0}_d,\Sigma_{\mu} = \bm{I}_d)$ as 
    prior parameters for $\mu$, and $(\nu = d + 50,\Psi = \nu I_d)$ as prior 
    parameters for $\Sigma$, except for the categorical components of the shape 
    vector as described in Section~\ref{sec:categorical}.  Deviations in $\mu_0$ 
    towards the negative direction bias the model towards asymptotic 
    independence, which in our testing resulted in lower model fidelity.
    To update the cluster shape vectors, we employ a joint proposal step in 
    log-space using a multivariate normal proposal, where the proposal 
    covariance is informed with an adaptive Metropolis 
    algorithm.\citep{Haario2001}.  To hasten updates to the shape
    parameters, and speed convergence of the model, we employ a parallel
    tempering algorithm where parallel MCMC chains are sampled at an ascending
    temperature ladder, where density is exponentiated to the reciprocal of the
    chain temperature: $f_t(\theta) = f(\theta)^{1/t}$.  Chains with 
    higher temperatures have flatter posteriors, and thus more readily move 
    around the parameter space.  Chain states are randomly exchanged via a 
    Metropolis step with probability
    $p_{1,2} = \exp\left\lbrace (t_2^{-1} - t_1^{-1})(L_2-L_1)\right\rbrace$,
    where $t$ refers to chain temperature, and $L$ refers to the
    \emph{energy}, or log-density of the chain at its current state.
    The sample history of the cold chain, where $t:=1$, is preserved as draws
    from the posterior distribution.
    For each example dataset, the sampler was ran for $50,000$ iterations,
    discarding the first 40,000 as burn-in.  The resulting chain was thinned,
    keeping only every 10th iteration.  For evaluating density under the 
    posterior predictive distribution, we generate 10 replicates from each
    iteration kept.

\begin{table}[t]
    \centering
    \caption{
    Characteristics of datasets used in the analysis.  For a given model, 
    $N$ and $A$ refer to the number of observations and anomalies in the 
    fitting set, respectively. $M$ identifies the number of categorical 
    variables, with $d_{\bm{v}}$ and $d_{\bm{w}}$ identifying the total number 
    of real and categorical columns respectively.  $t$ is the time (in hours) 
    to fit the model.  Discrepancy in $d$ between peaks--over--threshold 
    and rank--transformation reflects differences in data transformation,
    as well as the additional column for the radial component in the
    rank--transformed model.\label{tab:data}
    }
    \bigskip
    \scriptsize
    \setlength\tabcolsep{3pt}
    \input {./tables/data_summary}
    % \makenote{Here lies a table of characteristics of datasets used in analysis.  
    % Imagine first column dataset name, second column number of obsv. original, third 
    % threshold; 4th prevalence of anomalies/above threshold, fourth number of obsv. 
    % after thresholding (include whether subsampling was used), 5th number of obsv. 
    % included in rank (include whether subsampling was used); 6th number of obsv. 
    % included in categorical (include whether subsampling was used).  that's too 
    % many columns.  need to work on this.}
    \setlength\tabcolsep{3pt}
\end{table}

We compared our four proposed scores against three canonical novelty detection 
    algorithms, including isolation forest \emph{(iso)} \cite{liu2000}, local 
    outlier factor (lof) \cite{breunig2000}, and one-class SVM (svm) 
    \cite{chang2011}.  Each dataset was subject to 5-fold cross-validation, and 
    out-of-sample performance scores were averaged to produce the resulting 
    performance tables seen in this section.  This additional step of 
    cross--validation turned out to be unnecessary for our model, as 
    out--of--sample performance did not markedly differ from in--sample or 
    full--sample performance for the tested datasets.  Table~\ref{tab:data}
    provides a summary description of the datasets used in the analysis.
    \makenote{should the ordering of regimes in the table reflect the 
    ordering in the table?  Also, $N$ and $A$ between rank and categorical are
    the same.  Should they be combined?}
    For larger datasets, we subsetted the raw data to reduce computation time
    for the rank--transformation and categorical applications.
    Note that the categorical versions of \emph{cover}, \emph{pima}, and 
    \emph{yeast} are created from discretizing the rank--transformation subsets.
    First, we present score efficacy on our purely categorical data model, then 
    mixed scoring with thresholding on continuous variables.  Finally we present 
    mixed scoring on rank-transformation data.

\subsection{Categorical anomalies}
The categorical transformation of \emph{cover}, \emph{pima}, and \emph{yeast}
    discretized the real-valued and ordinal variables in those datasets.  For 
    \emph{cover} in particular, it seems this transformation lost a significant 
    amount of data.  From Table~\ref{tab:data}, it seems a large portion of 
    data regarding anomalies is contained within the radial component, so a 
    categorical transformation loses that information.  Likely for this reason, 
    none of the methods offer exceptional performance on this dataset.
\begin{table}[ht]
    \centering
    \small
    \caption{Area under the \emph{ROC} curve for various anomaly detection 
        schemes, on \emph{strictly categorical} datasets.  Reported here is 
        arithmetic mean of out-of-sample performance for 5-fold cross-validation.  
        Values closer to 1 are preferred.\label{tab:perfcat}}
    \bigskip
    \input{./tables/ad_cat_performance_update}
\end{table}
The dataset \emph{solarflare} was also unique in our analysis, being the only 
    truly categorical dataset used.  Our algorithm \emph{lskde} very slightly 
    trailed the performance of \emph{one-class SVM}, the best performing 
    algorithm on this dataset.  On both \emph{pima} and \emph{yeast}, 
    latent-simplex KDE performed significantly better than any of the canonical 
    methods.  On this analysis, \emph{hkde} and \emph{hknn} both performed 
    poorly.  It seems the projection of the categorical probability vectors into 
    a unified sphere induces some loss of information.

\subsection{Peaks-over-Threshold anomalies}
We subjected six datasets to multivariate thresholding, only keeping 
    observations that exceeded the threshold in at least one dimension.  For 
    most datasets, this threshold was set at $\hat{F}_{\ell}^{-1}(0.95)$ for 
    each dimension.  For \emph{cover}, we sub-sampled the data to produce a 
    more manageable sized dataset.  For variables on these datasets that did 
    not exhibit properties that would allow for a peak-over-threshold model to 
    apply, these variables were instead converted to discrete values with two 
    or three categories.  We built the mixed data model, and evaluated 
    performance of the mixed scores, compared against the canonical methods.  
    Of particular interest here is the \emph{annthyroid} dataset, for which all 
    of our scores performed comparably, and significantly better than the 
    canonical scores.  Of the other tested datasets, on \emph{cardio}, 
    \emph{lmkde} approached the performance of \emph{isolation forest} and 
    \emph{one-class SVM}, but all other methods performed worse.  For the 
    datasets \emph{cover} and \emph{mammography}, \emph{hknn}, \emph{lhkde}, 
    and \emph{lmkde} performed comparably, and each significantly better than 
    any of the canonical methods.  We see that \emph{lmkde}, being the inheritor
    of the latent simplex KDE score, performs reasonably well reliably among 
    datasets thus far in the peaks-over-threshold setting, but is outperformed 
    by other metrics on each dataset. We may see some effect of the loss of 
    information relating to the dependence structure between $\bm{w}$ and 
    $\bm{v}$ on the derived performance.  On that note, \emph{lhkde} performed 
    comparably to \emph{lmkde} on \emph{annthyroid}, \emph{cover}, \emph{pima}, 
    and \emph{yeast}, but slightly exceeded its performance on 
    \emph{mammography}.  We saw in the categorical datasets, \emph{lskde} 
    performed generally well, so the projection onto a unified sphere may 
    induce loss of information.  In that regard, it may be the case that 
    preserving information about the dependence structure between $\bm{v}$ and 
    $\bm{w}$ had a greater effect than a greater effect than preserving 
    information within $\bm{w}$ specifically.
    
\begin{table}[ht]
    \centering
    \caption{Area under the \emph{ROC} curve for various anomaly detection 
        schemes, on \emph{mixed} data where the real component has undergone 
        the \emph{threshold} standard Pareto transformation.   Reported here is 
        arithmetic mean of out-of-sample performance for 5-fold 
        cross-validation.  Values closer to 1 are preferred.
        \label{tab:perfreal}}
    \bigskip
    \small
    \input{./tables/ad_real_performance_update}
\end{table}

As to the poor performance of every method on \emph{pima} and \emph{yeast}, 
    these reported \emph{AuROC} values are conditional on the data exceeding 
    the multivariate threshold used in building the model.  As we see in 
    Table~\ref{tab:data}, these datasets do not meet the assumption that 
    anomalies are concentrated in the tails.  Scores depending on $r_i$, the
    radius component of $\bm{z}_i$, or \emph{magnitude} of the extremal 
    observation, are going to perform poorly relative to metrics that do not 
    make that assumption.

\subsection{Rank Transformation anomalies}
We subjected the same six datasets used in the peak-over-threshold model to 
    rank transformation on the real and ordinal variables. We then built the 
    mixed model including radius described in Section~\ref{subsec:rank} on the 
    transformed datasets.  Large datasets used in rank-transformation and 
    categorical models were sub-sampled to reduce computation time. 
    Note that rank transformation preserves the entire dataset, so we should 
    not consider the values in Table~\ref{tab:perfrank} to be comparable to 
    the values in Table~\ref{tab:perfreal}.
    
\begin{table}[ht]
    \centering
    \caption{Area under the \emph{ROC} curve for various anomaly detection 
        schemes, on \emph{mixed} data where the real component has undergone 
        the \emph{rank} standard Pareto transformation. Reported here is 
        arithmetic mean of out-of-sample performance for 5-fold 
        cross-validation.  Values closer to 1 are preferred.
        \label{tab:perfrank}}
    \small
    \bigskip
    \input{./tables/ad_rank_performance_update}
\end{table}

Here \emph{lmkde} performs better than each of the canonical methods in four of 
    six datasets, performing slightly worse than \emph{one-class SVM} on 
    \emph{mammography}, and significantly worse than \emph{isolation forest} on 
    \emph{yeast}.  As we have stated before, \emph{yeast} and \emph{pima} are 
    datasets that do not quite meet our assumptions as to how anomalies are 
    distributed, but our methods still make a strong showing on \emph{pima}.
    

% EOF