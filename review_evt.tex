    \section{Review of Extreme Value Theory\label{sec:evt}}

The use of methods based on extreme value theory to perform anomaly detection
    has the advantage of focusing on the tail of the distribution, which is 
    where anomalous observations are more likely to be found. In the univariate 
    case, asymptotic results provide a unique parametric limiting extreme 
    family. Software to infer the parameters of such family is widely available 
    \cite[see, for example,][]{coles2001}. A popular approach to study 
    univariate extremes, is to consider the observations  that exceed a 
    threshold, and calculate the excess values, then use them for inference on 
    the parameters of the generalized Pareto distribution that correspond to 
    the theoretical limit. This is known as  the peaks over threshold approach 
    (PoT), and will be central to the methods in this paper. In the multivariate 
    case the theory for PoT is well developed 
    \citep[see, for example][]{dehaan2006}, and it indicates the existence of a 
    limiting  distribution  that has no parametric representation. This 
    presents a challenge for inference. Furthermore, the difficulty of using a 
    PoT approach is compounded by the fact  that there is no unique definition 
    of an exceedance of a multivariate threshold, as there is an obvious 
    dependence on the norm used to measure the vectors sizes.

The multivariate PoT model considered in this paper has been developed in 
    \cite{trubey:pg}, and it is based on a representation of the limiting 
    distribution proposed in \cite{RoSeWa2018a}.  Let 
    $\bm{W} = (W_1, \ldots ,W_d)$ be a $d$-dimensional random vector with
    cumulative distribution $F$. Assume 
    that there exist sequences of vectors $\bm{a}_n$ and $\bm{b}_n$,
    and a $d$-variate distribution $G$ such that 
    $\lim_{n\rightarrow\infty} F^n(\bm{a}_n \bm{w} + \bm{b}_n) = 
    G(\bm{w})$. $G$ is a $d$-variate generalized extreme value 
    distribution. Then
    \begin{equation*}
        \begin{aligned}
            \lim\limits_{n\rightarrow\infty} 
                &\prob{\bm{a}^{-1}_n (\bm{W} - \bm{b}_n) 
                \leq \bm{w} \mid \bm{W} \not\leq \bm{b}_n)}  
                = \frac{\log G(\bm{w} \wedge \bm{0}) 
                - \log G(\bm{w}) }{\log G(\bm{0})} = H(\bm{w}).
        \end{aligned}
    \end{equation*}
    where $H$ is the multivariate Pareto distribution. \cite{RoSeWa2018a}
    provides a number of stochastic representations for $H$. In particular
    Remark 1 justifies the representation given in \cite{ferreira2014},
    consisting of taking $\bm{W}$, in the limit, as $\bm{W} = R\bm{V}$ 
    where $R$ and $\bm{V}$ are independent. $R = ||\bm{W}||_\infty$ is 
    distributed as a standard Pareto  random variable, and
    $\bm{V} = \bm{W}/||\bm{W}||_\infty$ is a random vector in  
    $\mathbb{S}_{\infty}^{d-1}$, the positive orthant of the 
    unit  sphere in infinite norm, with distribution $\Phi$. 
    $R$ and $\bm{V}$ are referred, respectively, as the {\em radial} 
    and {\em angular}  components of $H$. The angular measure controls 
    the dependence  structure of $\bm{W}$  in the tails. In view of 
    this, to obtain a  PoT model we seek a flexible model for the 
    distribution of $\bm{V} \in {\mathbb S}_{\infty}^{d-1}$. Our approach
    consists of a two-step analysis. We first standardize and subsample using
    a multivariate threshold. Then we estimate the angular measure.

Starting with a collection of observations 
    $\bm{w}_i \in {\mathbb R}^d, i = 1, \ldots, n$, we perform thresholding 
    for each marginal. We use as threshold 
    $b_{q,l} = \hat{F}^{-1}_{\ell}(1 - 1/q)$, where $\hat{F}_\ell$ is the 
    empirical distribucion function for the $\ell$th component. $t$ is 
    chosen to obtain a large empricial quantile, like 85 or 90\%. Thresholded 
    values are assumed to follow a generalized univariate Pareto distribution, 
    and are used to estimate the corresponding scale and shape parameters 
    $a_\ell$ and $\xi_\ell$. We then obtain the standardization
    \begin{equation}
        \label{eqn:standardization}
        z_{i\ell} = \left(1 + \xi_{\ell}\frac{w_{i\ell} -
            b_{\ell}}{a_{\ell}}\right)_{+}^{1/\xi_{\ell}}
    \end{equation}
    where $(\cdot)_+$ indicates the positive parts function. Let 
    $r_i = ||\bm{z}_i||_\infty$ and let $\bm{v}_i = \bm{z}_i/r_i$. Due to the 
    thresholding, $i$ ranges from 1 to $m\leq n$, and $r_i>1$.  That is, all 
    vectors have at least one very large component.

Recall that the radial component $R \in {\mathbb R}_+$ follows a standard 
    Pareto distribution, we focus on describing the distribution of the angular 
    component $\bm{V}\in {\mathbb S}_{\infty}^{d-1}$. For this purpose we use 
    the samples $\bm{v}_i ,\; i=1, \ldots ,m$. A suitable distribution for 
    $\bm{V}$ can be approximated by projecting a distribution in 
    $\mathbb{R}_+^d$ onto $\mathbb{S}_{p}^{d-1}$. Recall that the 
    $\mathcal{L}_p$ norm is 
    $\lVert \bm{s} \rVert_p = (\sum_{\ell = 1}^d \rvert s\lvert_{\ell}^p)^{1/p}$. 
    For $\bm{x} \in\mathbb{R}_+^d$, we define the transformation 
    \[
        T_p(\bm{x}) = (\lVert\bm{x}\rVert_p, x_1/\lVert\bm{x}\rVert_p, \ldots, 
        x_{d-1}/\lVert\bm{x}\rVert_p)=: (r,\bm{y}),
    \] where
    $\bm{y} = (y_1, \ldots , y_{d-1}) \in \mathbb{S}_{p}^{d-1}$, $r>0$, and 
    $y_d = \left(1 - \sum_{\ell = 1}^{d-1}y_\ell^p\right)^{\frac{1}{p}}$.
    Thus, a distribution on $\bm{X}$ induces a distribution on 
    $\mathbb{S}_{p}^{d-1}$. In particular, assume that $\bm{X}$ has independent 
    components, each of them distributed as 
    $\mathcal{G}a(\alpha_{\ell}, \beta_{\ell})$, then it can be seen that the 
    density of $\bm{y}$ is
    \begin{equation}
        \label{eqn:projgamma}
        f(\bm{ y}\mid\bm{ \alpha},\bm{ \beta}) =
            \prod_{\ell = 1}^d\left[
                \frac{\beta_{\ell}^{\alpha_{\ell}}}{\Gamma(\alpha_{\ell})}
                y_{\ell}^{\alpha_{\ell} - 1}
                \right]
            \left[y_d +
                {\textstyle \sum}_{\ell = 1}^{d-1}y_{\ell}^py_d^{1 - p}
                \right]
            \frac{\Gamma({\textstyle\sum}_{\ell = 1}^d\alpha_{\ell})}{
                    \left({\textstyle\sum}_{\ell = 1}^d
                \beta_{\ell}y_{\ell}\right)^{
                    {\scriptstyle\sum_{\ell = 1}^d \alpha_{\ell}}}
                },
  \end{equation}
    \citep[see,][for details]{trubey:pg}. This distribution is a generalization 
    to $\mathbb{S}_{p}^{d-1}$ of the projected gamma distribution defined for 
    $p=2$ in \cite{nunez2019}. We will denote its density as 
    $\mathcal{PG}_p(\bm{y}|\bm{\alpha},\bm{\beta})$.

% \begin{algorithm}[ht]
%     \caption{Workflow}
%     \KwData{$\bm{v}_i$, $i \in \lbrace1,\ldots,n\rbrace$}
%     \KwResult{Replicates of $\bm{V}$}
%     
% \end{algorithm}
\begin{algorithm}[tb]
    \caption{Workflow to fit a distribution to data 
        on $\mathbb{S}_{\infty}^{d-1}$.}\label{alg:workflow}
    % \begin{enumerate}[nolistsep]
        % \item
    \begin{algorithmic}[1]
        \State $b_{t\ell} := \hat{F}_{\ell}^{-1}\left(1 - \frac{1}{t}\right)$; 
            For $w_{i\ell} > b_{t\ell}$, fit $a_{\ell}$, $\xi_{\ell}$ 
            by likelihood-based method.
        % \item 
        \State Transform 
            $\bm{z}_{i\ell} = \left(1 + \xi_{\ell}\frac{w_{i\ell} 
                - b_{t\ell}}{a_{\ell}}\right)_+$ for $\bm{w}_i\not< \bm{b}_t$.
        %\item 
        \State Transform $(r_i,\bm{v}_i) = \left(\lVert \bm{z}_i\rVert_\infty, 
            \frac{\bm{z}_i}{\lVert \bm{z}_i\rVert_{\infty}}\right)$.
        % \item 
        \State $\bm{y}_i = \frac{\bm{z}_i}{\lVert \bm{z}_i\rVert_p}$ is used to facilitate fitting $f(\bm{\alpha}, \bm{\beta}\mid\bm{y})$.
        % \item 
    % \end{enumerate}
    \end{algorithmic}
\end{algorithm}

Note that $T$ is not differentiable at $p = \infty$, so we can not use it to 
    directly model on ${\mathbb S}_{\infty}^{d-1}$. However, as $p$ increases,
    the surface $\mathbb{S}_p^{d-1}$ will approach $\mathbb{S}_{\infty}^{d-1}$.
    This means that a projected gamma built on $\mathbb{S}_p^{d-1}$ with a
    sufficiently large $p$ will serve to approximate a distribution on 
    $\mathbb{S}_{\infty}^{d-1}$.  This approximation is leveraged 
    in~\cite{trubey:pg} to obtain samples of a distribution on 
    ${\mathbb S}_{\infty}^{d-1}$. To obtain a flexible model for $\Phi$ we use 
    the projected gamma density as the kernel of a random measure mixture model, 
    based on the Pitman Yor ($\mathcal{PY}$) process introduced 
    in~\cite{perman1992}. Pitman-Yor processes are fully atomic random 
    measures that are specified by two parameters and a centering distribution.
    They can be formulated, using a stick-breaking representation 
    \citep{Ish-James2001}, as
    \[
        \text{Pr}(\alpha\mid\cdots) = \sum_{j = 1}^{\infty}p_j\delta_{\bm{\alpha}_j};
        \;\;\;\sum_{j = 1}^{\infty}p_j = 1, \;\;\; p_j := \chi_j \prod_{k = 1}^{j-1}(1 - \chi_k)
    \]
    where $\delta_{\bm{\alpha}_j}$ indicates a point mass at $\bm{\alpha}_j$,
    and $\bm{\alpha}_j$ are sampled independently from $G_0$. $\chi_j \sim \text{Beta}(1 - d,
    \eta+jd)$. $d\in[0,1), \eta>-d$ are refereed to as the discount and the concentration 
    parameters, respectively. Pitman Yor processes have the advantage over the more 
    commonly used Dirichlet processes \citep{Ferguson74} of including a 
    discount parameter along with the concentration parameter, allowing greater 
    control over the formation of new clusters.  A hierarchical formulation of 
    the model for observations 
    $\bm{y}_i\in \mathbb{S}_{p}^{d-1}$, $i=1,\ldots,n$, is
    \begin{equation}
        \label{eqn:modelsphere}
        \begin{aligned}
        \bm{y}_i \mid \bm{\alpha}_i &\sim \mathcal{PG}_p
                \left(\bm{y}_i\mid\bm{\alpha}_i, \bm{1}\right)\\
        \bm{\alpha_i} &\sim G\\
        G &\sim \mathcal{PY}\left(d, \eta, G_0\right)\\
        \end{aligned}
        ~\hspace{1cm}
        \begin{aligned}
        G_0 &= \mathcal{LN}_d\left(\bm{\alpha}\mid\bm{\mu},\Sigma\right)\\
        \mu &\sim \mathcal{N}_d\left(\bm{0},\bm{1}\right)\\
        \Sigma &\sim \mathcal{IW}_d\left(\nu, \Psi\right).
        \end{aligned}
    \end{equation}
    Here $\mathcal{LN}$ denotes a log-normal, $\mathcal{N}$ a normal, and 
    $\mathcal{IW}$ an inverse Wishart.  We refer to this model as a
    \emph{Pitman--Yor mixture of projected gammas} $(\mathcal{PYPG})$.
    Details of the implementation of this 
    model using and adaptive MCMC approach are provided in the supplementary 
    material.  As a kernel density, it was observed in \cite{trubey:pg} that 
    the unrestricted form of the $\mathcal{PG}_p$ with both shape and rate 
    parameters offered no improvement in model fidelity on real data compared to 
    the restricted form, where the rate parameters are fixed at 1.  For a more 
    parsimonious model, and for compatibility with the categorical model
    that will be developed in Section \ref{sec:categorical}, in 
    this paper we choose to use the restricted form.

Mixtures of Pitman--Yor processes can be used to group 
    observations into stochastically assigned clusters, where all observations
    within a cluster share a set of parameters.  Cluster assignment is accomplished through data 
    augmentation, where a cluster identifier $\varphi_i$ is sampled according to
    both cluster weight and kernel density of observation $i$ given cluster parameters.
    We make use of the blocked-Gibbs sampler on a truncated stick-breaking representation of the
    Pitman--Yor model.  Cluster weights are then sampled as
    \begin{equation}
        \label{eqn:clusterweight}
        \chi_{j} \mid n_j, n_{k>j} \sim \text{Beta}\left(1 + n_j - d, 
            \eta + \sum_{k = j + 1}^J n_k + jd\right);
        \hspace{1cm}p_j := \chi_j \prod_{k = 1}^{j-1}(1 - \chi_k)
    \end{equation}
    where $n_j$ is the number of observations in cluster $j$.
    In this form, the Dirichlet process is a special case of the Pitman-Yor process
    where the discount parameter $d := 0$.
    Then the cluster identifier for observation $i$, $\varphi_i$, is sampled as
    \begin{equation}
        \label{eqn:clusterid}
        \text{Pr}\left[\varphi_i = j\mid \bm{p},\bm{\alpha}\right] =
            \frac{p_j\;\mathcal{PG}_p\left(\bm{y}_i\mid\bm{\alpha}_j,\bm{1}\right)}{
                \sum_{k = 1}^J p_k\;
                \mathcal{PG}_p\left(\bm{y}_i\mid\bm{\alpha}_k,\bm{1}\right)}.
    \end{equation}
    Within the blocked-Gibbs algorithm, $\bm{\chi}\mid\bm{\varphi}$ are mutually
    independent, as are $\bm{\varphi}\mid\bm{\chi}$.  This conditional independence
    offers an opportunity for parallelization, increasing the speed of sampling.
    
The approach proposed in this section produces a sample of the angular measure 
    of the distribution of the tails of the sample. The method has a number of 
    advantages for anomaly detection: it focuses on the tails, which is where we 
    are more likely to find anomalous behavior;  it accounts for asymptotic 
    dependence between the different components of the observation vector; it
    reduces the computational burden, by thinning the sample using thresholding; 
    and it decouples the radial component to the angular component, thanks to 
    independence. 
    % Samples of the radial component can easily be obtained from a 
    % standard Pareto.
    % \makenote{We never use this though.  Why mention?}

% EOF