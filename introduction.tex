\section{Introduction}

Anomaly detection, describes a field of methods for identifying observations as 
    \emph{anomalous}. This classification requires some explanation as to 
    \emph{what} defines an observation as anomalous. For this paper, following 
    the general trend of literature, we define anomalies as observations that 
    are in some manner \emph{different} than non-anomalous data. We interpret 
    this to say that anomalies were not generated according to the same 
    generating distribution as non-anomalous data, and as such, we would expect 
    observations found in regions of relative data sparsity to be more likely to 
    be anomalous than those observations found in regions of high data 
    abundance.  We characterize this assumption as \emph{anomalies stand apart}.
    In literature as here, the term \emph{normal data} is used to refer to data 
    which is not anomalous.  Normal data tend to cluster into homogenous groups, 
    but anomalous data are heterogenous in their differences. 
    \makenote{This is a paraphrase of a similar statement.  Looking for source.}
    
Equivalent names for this field include \emph{outlier} detection, and 
    \emph{novelty} detection, though these terms include their own nuances.
    Outliers are characterized as observations that are in some manner 
    \emph{far} from \emph{normal} data.  In a regression context, they may have
    large fitted residuals, or exert large influence on model fits.  Novelties
    in contrast are data coming from a distribution that has not been seen
    before.  A novelty detection application will then assume a \emph{clean} 
    training data set containing no anomalies, and identify observations not
    belonging to the distribution as trained.  \cite{Chandola2009} refer to 
    this practice as \emph{semisupervised} anomaly detection.

The applicability of extreme value theory to anomaly detection is predicated on 
    the assumption that extreme observations are more likely to be anomalous.  
    A discussion on this point is provided by \cite{goix2017}, stating that 
    extreme observations exist at the border between anomalous and non-anomalous 
    regions.  Indeed, for most datasets in our testing, the probability an 
    individual observation is anomalous is higher for data in the tails of the 
    distribution. This relative abundance of anomalies among extremes might 
    cause a naive classifier that does not take into account the dependence 
    structure of extremes to classify all extremes as anomalous.  If we follow 
    the assumption that anomalies stand apart, then extreme observations that 
    cluster into a homogenous group should not be considered anomalous.  For 
    this reason, we desire a classifier that considers the dependence structure 
    of the extremes as well.

For our purpose, we do not assume the existence of labels in the training 
    dataset, and seek an algorithm that can produce anomaly scores in an absence 
    of class labels. As such, we will offer a brief overview of unsupervised 
    anomaly detection methods, as well as discussion of the methods we are 
    preparing here as competing models.

% EOF






