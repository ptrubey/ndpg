\section{Introduction}

Anomaly detection, describes a field of methods for identifying observations as 
    \emph{anomalous}; % which is 
    a term that requires defining. 
    For this paper, following 
    the general trend in the literature, we define anomalies as observations that 
    are in some manner different than non-anomalous data. We interpret 
    this to say that anomalies are data that were not produced by the same 
    generating distribution as non-anomalous data, and as such, we would expect 
    observations found in regions of relative data sparsity to be more likely to 
    be anomalous than those observations found in regions of high data 
    abundance.  We characterize this assumption as \emph{anomalies stand apart}.
    In the literature as here, the term \emph{normal data} is used to refer to data 
    which are not anomalous.  Normal data tend to cluster into homogenous groups, 
    but anomalous data are heterogenous in their differences. 
    %\makenote{This is a paraphrase of a similar statement.  Looking for source.}
    
Alternative names for the field of anomaly detection include \emph{outlier} detection, and 
    \emph{novelty} detection, though these terms have their own nuances.
    Outliers are characterized as observations that are in some manner 
    far from normal data.  In a regression context, they may have
    large fitted residuals, or exert large influence on model fits.  Novelties
    in contrast are data coming from a distribution that has not been seen
    before.  A novelty detection application will then assume a clean 
    training data set containing no anomalies, and identify observations not
    belonging to the distribution as trained.  \cite{Chandola2009} refer to 
    this practice as semi-supervised anomaly detection. For our purpose, we do 
    not assume the existence of labels in the training 
    dataset, and seek an algorithm that can produce anomaly scores in the absence 
    of class labels. As such, we will offer a brief overview of unsupervised 
    anomaly detection methods, as well as discussion of the methods we are 
    proposing here as competing models.
    
    % \st{Core to the 
    % development of a method lies a basic the question of what is an anomaly.  
    % The answer to this question depends on what we hope to
    % learn in a specific application. Common to most 
    % methods of anomaly detection is the belief that anomalous data is in some 
    % manner different than the rest of the data. This belief implies that data 
    % belonging to regions of relative data sparsity are more likely to be 
    % anomalous, and thus, the goal is to identify the regions of relative 
    % data sparsity. It is common in most approaches not to assume the existence 
    % of labels in the training dataset, meaning anomaly detection is an 
    % unsupervised learning problem.  Instead we seek anomaly \emph{scores}, 
    % which rank observations in terms of their likelihood of being anomalous.  The 
    % decision threshold---the score value above which observations would be 
    % declared anomalous---is then set heuristically.}

%The applicability of extreme value theory to anomaly detection is predicated on 
%    the assumption that extreme observations are more likely to be anomalous.  
%    A discussion on this point is provided by \cite{goix2017}, stating that 
%    extreme observations exist at the border between anomalous and non-anomalous 
%    regions.  Indeed, for most datasets in our testing, the probability an 
%    individual observation is anomalous is higher for data in the tails of the 
%    distribution. This relative abundance of anomalies among extremes might 
%    cause a naive classifier that does not take into account the dependence 
%    structure of extremes to classify all extremes as anomalous.  If we follow 
%    the assumption that anomalies stand apart, then extreme observations that 
%    cluster into a homogenous group should not be considered anomalous.  For 
%    this reason, we desire a classifier that considers the dependence structure 
%    of the extremes as well.

% EOF






